{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter definitions and imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters to be changed by user\n",
    "It should be possible to run the code after setting only the variables in the cell below. In the provided directory names, '{group}' will be replaced by immediate/delay for the two groups. For example /my_data_dir/{group}/batch_data will be replaced by two directories, one called /my_data_dir/immediate/batch_data and one called /my_data_dir/delay/batch_data and the data from each group will be put in the corresponding directory.\n",
    "File names have default names but can be changed as well in the code below\n",
    "Assumes all directories exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Locations of the files that have been downloaded from OSF\n",
    "#\n",
    "\n",
    "# The directory where the python code is placed (the file called stopmotion_funcs.py should be placed here)\n",
    "code_dir = '/code_dir/'\n",
    "\n",
    "# The directory where the R code is placed (the file called create_brms_models.R should be placed here)\n",
    "R_dir = '/R_dir/'\n",
    "\n",
    "# The directory holding all the logs of the experiment before processing (as saved from JATOS). The\n",
    "# only processing the files have undergone is the replacement of the original participant ID from \n",
    "# prolific with an experiment-specific one (the contents of the 'logs' directory on OSF should be placed here)\n",
    "log_dir = '/log_dir/logs/'\n",
    "\n",
    "# File with the time of each action (relative to movie start), downloaded with experiment data\n",
    "action_time_fname = '/downloaded_data_dir/action_timings.csv'\n",
    "\n",
    "# Directory holding all the event segmentation logs, and the sub-directories for the neutral/\n",
    "# surprise version. The event seg logs are in separate logs for each participant\n",
    "seg_data_dir = '/seg_data_dir/'\n",
    "seg_subdir = 'neutral/'\n",
    "seg_S_subdir = 'surprise/'\n",
    "\n",
    "\n",
    "#\n",
    "# Empty directories that the code will place files in - these directories are expected to already exist\n",
    "#\n",
    "\n",
    "# A directory in which to place summary files for each batch (including overall subject performance, \n",
    "# the duration devoted to each section and a summary of the debriefing)\n",
    "batch_summary_dir = '/my_data_dir/{group}/batch_summaries/'\n",
    "\n",
    "# A directory in which to place more detailed files which are the result of the batch processing\n",
    "batch_data_dir = '/my_data_dir/{group}/batch_data/'\n",
    "\n",
    "# A directory in which to place files that are common to both groups\n",
    "comb_data_dir = '/my_data_dir/combined/'\n",
    "    \n",
    "# A directory in which to put the brms models (for single-trial analysis). The data will also be saved here\n",
    "brms_dir = '/brms_dir/'\n",
    "\n",
    "\n",
    "#\n",
    "# Additional params\n",
    "#\n",
    "\n",
    "# dpi used for plots (reduce for faster plotting)\n",
    "plot_dpi = 75\n",
    "\n",
    "# flag indicating whether to save the figures and a directory in which to save them \n",
    "# (can be left empty if flag set to false)\n",
    "save_figs = False\n",
    "fig_dir = '/fig_dir/'\n",
    "\n",
    "# various figure size params (originally quite large for saving figures, left as reference)\n",
    "fig_w = 6 #12\n",
    "fig_h = 5 #10\n",
    "label_font = 14 #28\n",
    "tick_font = 12 #24\n",
    "title_font = 16 #32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "System imports, import of the python code that processes logs (process_batch.py) and loading of rpy2 extension to allow incorporating R code in notebook. Adds also a dedicated R cell for imports in R, so that in the rest of the code the R cells are independent of one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All required imports\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(code_dir)\n",
    "from stopmotion_funcs import process_batch, calc_surprise_dist, process_event_seg\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(dplyr)\n",
    "library(plyr)\n",
    "library(BayesFactor)\n",
    "library(brms)\n",
    "library(rslurm)\n",
    "library(lme4)\n",
    "library(lmerTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional parameter definitions\n",
    "These definitions do not require changing by user, as they are fixed for this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Information regarding the debriefing questions - the total number of debrief questions and the first one\n",
    "# including a survey (free text area)\n",
    "n_debrief_q = 7\n",
    "first_debrief_survey = 6\n",
    "\n",
    "batch_n_subjects = 24 # Number os participants in each batch\n",
    "\n",
    "batch_num = 8 # The batch number currently processed (8 for entire experiment)\n",
    "max_n_batch = 8 # Maximal number of batches to run\n",
    "\n",
    "# Criteria for subject exclusion - minimal accuracy (Pr) in validation questions \n",
    "# and minimal number of correct catch trials\n",
    "validation_min_Pr = 0.75\n",
    "catch_min_correct = 8\n",
    "\n",
    "# Bayes Factor threshold for stopping the batches. Once this threshold is reached in favour of the \n",
    "# alternative (H1) for one of the tests or in favour of the null (H0) in all tests, that will indicate\n",
    "# the criterion has been reached\n",
    "bf_thresh = 6\n",
    "\n",
    "# Names of the R script used for the single-trial analysis\n",
    "create_models_fname = 'create_brms_models.R'\n",
    "\n",
    "# Parallelisation parameters for the exploratory analysis\n",
    "n_nodes = 10\n",
    "cpus_per_node = 20\n",
    "n_iter = 1000\n",
    "\n",
    "#\n",
    "# Event segmentation parameters\n",
    "#\n",
    "\n",
    "demo_boundaries = np.array([4042, 6708, 11042]) # timings of color-changes in demo\n",
    "demo_length = 14000\n",
    "\n",
    "# Timings of scene changes in each film\n",
    "mov_scene_change =  [[12167, 51500, 69833, 105167, 122167],\n",
    "        [10333, 29333, 40000, 58667, 71333, 87333, 92833, 103833, 131000, \n",
    "            152500, 164167, 182833, 200000, 213667, 230333, 240500, 262833],\n",
    "        [11500, 46500, 56500, 74000, 106500, 128000, 169500, 189167, 216833, \n",
    "             230333, 260667, 268333, 304667, 333833, 350167]]\n",
    "# Timings of target actions in each film\n",
    "target_actions = [[29667, 78333, 138500],\n",
    "        [20833, 43333, 80833, 117167, 158500, 188000, 223333, 274833],\n",
    "        [29333, 88500, 151333, 208333, 238833, 313833, 355833]]\n",
    "\n",
    "# Lengths of the three movies in msec\n",
    "mov_lengths = np.array([152958, 289792, 376458])\n",
    "\n",
    "# Minimal distance between button presses to be considered two boundaries\n",
    "# (since long button presses can be logged multiple times)\n",
    "boundary_min_dist = 2000\n",
    "\n",
    "# Size of sliding window and jump (both in sec) for the boundary aggregation analysis\n",
    "# (will calculate the number of people identifying a boundary in each win, at each jump)\n",
    "seg_win_size = 3\n",
    "seg_jump = 1\n",
    "\n",
    "# Minimal number of participants that identified a boundary at each peak of the \n",
    "# sliding window analysis for that peak to be considered a boundary\n",
    "boundary_min_subj = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process log files\n",
    "Takes the log files, as saved by JATOS, and extracts the information to a more convenient format for analysis. Also saves the formatted variables in files so they can be later loaded or opened in excel (saved as tab-separated files). There is one log file per batch of each group (immediate/delay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets various \n",
    "# Organises the parameters in dictionaries as expected by the process_batch function. Also adds specific \n",
    "# filenames - default names are used for these, not set in parameters above.\n",
    "\n",
    "exp_params = {\n",
    "        'n_mov': len(mov_lengths),\n",
    "        'n_debrief_q': n_debrief_q,\n",
    "        'first_debrief_survey': first_debrief_survey,\n",
    "        'batch_n_subjects': batch_n_subjects,\n",
    "        'validation_min_Pr': validation_min_Pr,\n",
    "        'catch_min_correct': catch_min_correct\n",
    "}\n",
    "\n",
    "file_params = {\n",
    "        'results_temp': log_dir + '{group}_batch{batch_num}.txt',\n",
    "        'summary_dir': batch_summary_dir,\n",
    "        'debrief_temp': '{group}_batch{batch_num}_debrief.tsv',\n",
    "        'performance_temp': '{group}_batch{batch_num}_performance.tsv',\n",
    "        'durations_temp': '{group}_batch{batch_num}_durations.tsv',\n",
    "        'data_dir': batch_data_dir,\n",
    "        'comb_data_dir': comb_data_dir\n",
    "}\n",
    "\n",
    "# Loops over all batches until current one, loading the data from each. The process_batch\n",
    "# function will load each batch if it has already been processed, or process the batch if \n",
    "# it hasn't. In either case it will return the same variables.\n",
    "immediate_res = []\n",
    "delay_res = []\n",
    "for b in range(batch_num):\n",
    "    immediate_res.append(process_batch('immediate', b+1, exp_params, file_params))\n",
    "    delay_res.append(process_batch('delay', b+1, exp_params, file_params))\n",
    "\n",
    "    \n",
    "# Calculates the distance of each action from the preceding/following surprise, for each movie\n",
    "# version. This function relys on the q_info_df and surprise_ver_df created by process_batch\n",
    "# so must be run after process_batch has been run at least once. This df will be used to \n",
    "# run the secondary linear mixed model analysis\n",
    "surprise_dist_df = calc_surprise_dist(action_time_fname, immediate_res[0]['surprise_ver_df'], \n",
    "                                      immediate_res[0]['q_info_df'], comb_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creates variables that hold the aggregated data, across all batches. One dataframe for the average performance and one with the surprise/neutral difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenates the immediate and delay groups of all batches, creating one full dataframe\n",
    "avg_I = pd.concat([res['test_subj_avg_df'] for res in immediate_res])\n",
    "avg_D = pd.concat([res['test_subj_avg_df'] for res in delay_res])\n",
    "\n",
    "# The subject numbers are restarted in each batch for flexibility (e.g. concatenating \n",
    "# only dataframes of one group), so updates the subject number to run across all batches\n",
    "# of both groups, then concatenates both groups into a single dataframe\n",
    "avg_I['subj'] = avg_I['subj'] + (avg_I['batch']-1)*batch_n_subjects*2\n",
    "avg_D['subj'] = avg_D['subj'] + (avg_D['batch']-1)*batch_n_subjects*2 + batch_n_subjects\n",
    "full_subj_avg_df = pd.concat((avg_I, avg_D)).reset_index(drop=True)\n",
    "\n",
    "# Creates a version of the dataframe with the Neutral Pr subtracted from the Surprise Pr of each subject,\n",
    "# divide by question type\n",
    "full_S_df = full_subj_avg_df[full_subj_avg_df['surprise_type']=='S'].drop(\\\n",
    "                                columns=['n_hit', 'n_HC_hit', 'avg_conf', 'n_FA', 'n_HC_FA', \n",
    "                                'avg_lure_conf', 'surprise_type', 'n_miss', 'n_HC_miss', \n",
    "                                'n_nFA', 'n_HC_nFA', 'd', 'HC_d']).\\\n",
    "                                rename({'Pr': 'Pr-S', 'HC_Pr': 'HC_Pr-S'}, axis=1)\n",
    "full_N_df = full_subj_avg_df[full_subj_avg_df['surprise_type']=='N'].drop(\\\n",
    "                                columns=['n_hit', 'n_HC_hit', 'avg_conf', 'n_FA', 'n_HC_FA', \n",
    "                                'avg_lure_conf', 'surprise_type', 'n_miss', 'n_HC_miss', \n",
    "                                'n_nFA', 'n_HC_nFA', 'd', 'HC_d']).\\\n",
    "                                rename({'Pr': 'Pr-N', 'HC_Pr': 'HC_Pr-N'}, axis=1)\n",
    "full_diff_df = full_S_df.merge(full_N_df, how='outer', validate='one_to_one')\n",
    "full_diff_df['Pr_diff'] = full_diff_df['Pr-S']-full_diff_df['Pr-N']\n",
    "full_diff_df['HC_Pr_diff'] = full_diff_df['HC_Pr-S']-full_diff_df['HC_Pr-N']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counts and prints the number of participants excluded from each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First across all batches\n",
    "n_exc_I = full_subj_avg_df[(full_subj_avg_df['exc_subj'])&(full_subj_avg_df['group']=='I')]['subj'].nunique()\n",
    "n_exc_D = full_subj_avg_df[(full_subj_avg_df['exc_subj'])&(full_subj_avg_df['group']=='D')]['subj'].nunique()\n",
    "print(f'Number of excluded participants: {n_exc_I} in the immediate group and {n_exc_D} in the delay group')\n",
    "\n",
    "# Then for the first batch only\n",
    "n_exc_I_b1 = full_subj_avg_df[(full_subj_avg_df['exc_subj'])&(full_subj_avg_df['batch']==1)&(full_subj_avg_df['group']=='I')]['subj'].nunique()\n",
    "n_exc_D_b1 = full_subj_avg_df[(full_subj_avg_df['exc_subj'])&(full_subj_avg_df['batch']==1)&(full_subj_avg_df['group']=='D')]['subj'].nunique()\n",
    "print(f'Number of excluded participants in the first batch: {n_exc_I_b1} in the immediate group and {n_exc_D_b1} in the delay group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process event-seg logs\n",
    "Same as above, but here processes the event segmentation logs. There are two sets of logs, one group viewed the films with all scenes in their neutral version (presented in manuscript supplementary) and one group viewed the films with all scenes in their surprising version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organises the parameters in dictionaries as expected by the process_event_seg function. Also adds specific \n",
    "# filenames - default names are used for these, not set in parameters above.\n",
    "\n",
    "seg_params = {\n",
    "    'demo_boundaries': demo_boundaries,\n",
    "    'demo_length': demo_length,\n",
    "    'mov_scene_change': mov_scene_change,\n",
    "    'target_actions': target_actions,\n",
    "    'mov_lengths': mov_lengths,\n",
    "    'n_debrief_q': n_debrief_q,\n",
    "    'first_debrief_survey': first_debrief_survey,\n",
    "    'boundary_min_dist': boundary_min_dist,\n",
    "    'win_size': seg_win_size,\n",
    "    'boundary_min_subj': boundary_min_subj,\n",
    "    'jump': seg_jump\n",
    "}\n",
    "\n",
    "seg_file_params = {\n",
    "    'data_dir': seg_data_dir,\n",
    "    'results_temp': seg_subdir + 'jatos_results_*.txt',\n",
    "    'summary_fname': 'event_seg_performance_summary.tsv',\n",
    "    'debrief_fname': 'event_seg_debrief.tsv',\n",
    "    'durations_fname': 'event_seg_durations.tsv',\n",
    "}\n",
    "\n",
    "seg_S_file_params = {\n",
    "    'data_dir': seg_data_dir,\n",
    "    'results_temp': seg_S_subdir + 'jatos_results_*.txt',\n",
    "    'summary_fname': 'event_seg_S_performance_summary.tsv',\n",
    "    'debrief_fname': 'event_seg_S_debrief.tsv',\n",
    "    'durations_fname': 'event_seg_S_durations.tsv',\n",
    "}\n",
    "\n",
    "seg_res = process_event_seg(seg_params, seg_file_params)\n",
    "seg_S_res = process_event_seg(seg_params, seg_S_file_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First batch calculations\n",
    "Calculations from the 'Ceiling/floor performance' section of the manuscript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First batch - data organisation\n",
    "Organises the data (in python) for the R code that runs the actual calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a separate dataframe for the first batch\n",
    "first_batch_df = full_subj_avg_df[full_subj_avg_df['batch']==1]\n",
    "\n",
    "\n",
    "# Creates a version of the first batch dataframe with the Target-N Pr subtracted from the Target-S Pr\n",
    "target_diff_df = full_diff_df[(full_diff_df['batch']==1)&(full_diff_df['q_type']=='T')].\\\n",
    "                                    reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First batch - calculations (in R)\n",
    "R code that calculates:\n",
    "- Evidence (BF) in favour of no difference between accuracy on Target-S and Target-N questions\n",
    "- Evidence (BF) in favour of zero accuracy on non-target questions, divided by group\n",
    "- Number of participants with ceiling performance (Pr>0.9), divided by group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i first_batch_df,target_diff_df\n",
    "\n",
    "# Sets seed to arbitrary value at the beginning of each R cell, for reproducibility\n",
    "set.seed(26)\n",
    "\n",
    "# Removes excluded subjects from analysis\n",
    "first_batch_data <- subset(first_batch_df, exc_subj=='FALSE')\n",
    "target_diff_data <- subset(target_diff_df, exc_subj=='FALSE')\n",
    "\n",
    "# Calculates the BF in favour of the null for a T-S vs. T-N comparison. (ttestBF gives the evidence\n",
    "# in favour of the alternative - so derives the evidence in favour of the null). Runs a one-sided comparison,\n",
    "# testing the evidence for T-S>T-N\n",
    "target_bf <- ttestBF(as.numeric(unlist(target_diff_data['Pr_diff'])), nullInterval=c(0, Inf))\n",
    "target_bf_H0 <- 1/as.numeric(as.vector(target_bf[1]))\n",
    "\n",
    "# Calculates the evidence in favour of the null for accuracy in the non-target questions, \n",
    "# collapsing across surprising and neutral and across question type (since there's the \n",
    "# same number of events for each - simply averages the Pr from the subj average dataframe)\n",
    "\n",
    "non_target_data <- subset(first_batch_data, q_type!='T')   \n",
    "nontarget_comb_data_I <- subset(ddply(non_target_data, .(subj, group), summarise, mean_Pr = mean(Pr)),\n",
    "                                group=='I')\n",
    "nontarget_comb_data_D <- subset(ddply(non_target_data, .(subj, group), summarise, mean_Pr = mean(Pr)),\n",
    "                                group=='D')\n",
    "nontarget_bf_H0_I <- 1/as.numeric(as.vector(ttestBF(as.numeric(unlist(nontarget_comb_data_I['mean_Pr'])))))\n",
    "nontarget_bf_H0_D <- 1/as.numeric(as.vector(ttestBF(as.numeric(unlist(nontarget_comb_data_D['mean_Pr'])))))\n",
    "\n",
    "# Checks how many participants have a Pr>0.9 in the non-target questions\n",
    "n_subj_ceiling_I <- sum(nontarget_comb_data_I['mean_Pr']>0.9)\n",
    "n_subj_ceiling_D <- sum(nontarget_comb_data_D['mean_Pr']>0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First batch - display results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls the relevant variables from the R code and converts to python native data types\n",
    "\n",
    "%Rpull target_bf_H0 nontarget_bf_H0_I nontarget_bf_H0_D n_subj_ceiling_I n_subj_ceiling_D \n",
    "\n",
    "target_bf_H0 = np.asarray(target_bf_H0)[0]\n",
    "nontarget_bf_H0_I = np.asarray(nontarget_bf_H0_I)[0]\n",
    "nontarget_bf_H0_D = np.asarray(nontarget_bf_H0_D)[0]\n",
    "n_subj_ceiling_I = np.asarray(n_subj_ceiling_I)[0]\n",
    "n_subj_ceiling_D = np.asarray(n_subj_ceiling_D)[0]\n",
    "\n",
    "# Prints results of calculations\n",
    "print('Evidence of floor performance (immediate group): %.3g'% (nontarget_bf_H0_D))\n",
    "print('Evidence of floor performance (delay group): %.3g'% (nontarget_bf_H0_I))\n",
    "print('Percent of participants with ceiling performance (immediate group): ' + \\\n",
    "              '%d' % (100*n_subj_ceiling_I/batch_n_subjects))\n",
    "print('Percent of participants with ceiling performance (delay group): ' + \\\n",
    "              '%d' % (100*n_subj_ceiling_D/batch_n_subjects))\n",
    "print('Evidence of no difference between surprising and neutral targets (collapsed across groups): ' \\\n",
    "              + '%.3g' % (target_bf_H0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopping criterion calculations\n",
    "Calculates stopping criteria tests for data from all batches up to current batch number to see if the criterion has been reached. Calculations from the 'Sample size determination' section of the manuscript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping criterion - Bayes Factor calculations (R)\n",
    "Calculates the Bayes Factor in favour of the alternative and in favour of the null for each of the following criteria:\n",
    "- Main effect of surprise\n",
    "- Interaction between question type (preS/preT) and surprise (S/N)\n",
    "- 3-way interaction of question type, surprise and group (immediate/delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i full_subj_avg_df\n",
    "\n",
    "# Sets seed to arbitrary value at the beginning of each R cell, for reproducibility\n",
    "set.seed(26)\n",
    "\n",
    "# Takes only preS/preT questions for the stopping criteria analysis and removes all excluded subjects\n",
    "retroactive_data <- subset(full_subj_avg_df, (q_type=='preS' | q_type=='preT') & exc_subj=='FALSE')\n",
    "\n",
    "# Turns the subj column into a factor - will be converted to numeric by default\n",
    "# Also sets the others to be factors (though they're already strings), since something\n",
    "# in the conversion in the notebook works differently and at least one isn't recognised \n",
    "# as a factor otherwise\n",
    "retroactive_data[,'subj'] <- as.factor(retroactive_data[,'subj'])\n",
    "retroactive_data[,'q_type'] <- as.factor(retroactive_data[,'q_type'])\n",
    "retroactive_data[,'surprise_type'] <- as.factor(retroactive_data[,'surprise_type'])\n",
    "retroactive_data[,'group'] <- as.factor(retroactive_data[,'group'])\n",
    "\n",
    "# Runs a 3-way anova with question-type, surprise and group\n",
    "retro_bf = anovaBF(Pr ~ q_type*surprise_type*group + subj, data=retroactive_data, whichRandom='subj')\n",
    "\n",
    "# Extracts BF in favour of the alternative (H1) and the null (H0) for each of the criteria\n",
    "bf_main_surprise_H1 = as.numeric(as.vector(retro_bf[9]/retro_bf[4]))\n",
    "bf_surprise_q_type_H1 = as.numeric(as.vector(retro_bf[17]/retro_bf[12]))\n",
    "bf_3way_interaction_H1 = as.numeric(as.vector(retro_bf[18]/retro_bf[17]))\n",
    "bf_main_surprise_H0 = 1/bf_main_surprise_H1\n",
    "bf_surprise_q_type_H0 = 1/bf_surprise_q_type_H1\n",
    "bf_3way_interaction_H0 = 1/bf_3way_interaction_H1\n",
    "\n",
    "# Makes sure the correct models were taken from the full anova (in case the order can change)\n",
    "models_validated <- TRUE\n",
    "models_validated <- models_validated & setequal(unlist(strsplit(names(as.vector(retro_bf[4])), ' + ', TRUE)), \n",
    "                        c('group','q_type','group:q_type','subj'))\n",
    "models_validated <- models_validated & setequal(unlist(strsplit(names(as.vector(retro_bf[9])), ' + ', TRUE)), \n",
    "                        c('group','q_type','group:q_type', 'surprise_type','subj'))\n",
    "models_validated <- models_validated & setequal(unlist(strsplit(names(as.vector(retro_bf[12])), ' + ', TRUE)),\n",
    "                        c('group','q_type','surprise_type', 'group:q_type','group:surprise_type','subj'))\n",
    "models_validated <- models_validated & setequal(unlist(strsplit(names(as.vector(retro_bf[17])), ' + ', TRUE)),\n",
    "                        c('group','q_type','surprise_type', 'group:q_type','group:surprise_type',\n",
    "                          'q_type:surprise_type','subj'))\n",
    "models_validated <- models_validated & setequal(unlist(strsplit(names(as.vector(retro_bf[18])), ' + ', TRUE)),\n",
    "                        c('group','q_type','surprise_type', 'group:q_type','group:surprise_type',\n",
    "                          'q_type:surprise_type','group:q_type:surprise_type', 'subj'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping criterion - display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls the relevant variables from the R code and converts to python native data types\n",
    "\n",
    "%Rpull models_validated bf_main_surprise_H1 bf_surprise_q_type_H1 bf_3way_interaction_H1 bf_main_surprise_H0 \\\n",
    "    bf_surprise_q_type_H0 bf_3way_interaction_H0\n",
    "\n",
    "models_validated = bool(np.asarray(models_validated)[0])\n",
    "bf_main_surprise_H1 = np.asarray(bf_main_surprise_H1)[0]\n",
    "bf_surprise_q_type_H1 = np.asarray(bf_surprise_q_type_H1)[0]\n",
    "bf_3way_interaction_H1 = np.asarray(bf_3way_interaction_H1)[0]\n",
    "bf_main_surprise_H0 = np.asarray(bf_main_surprise_H0)[0]\n",
    "bf_surprise_q_type_H0 = np.asarray(bf_surprise_q_type_H0)[0]\n",
    "bf_3way_interaction_H0 = np.asarray(bf_3way_interaction_H0)[0]\n",
    "\n",
    "# Determines whether stopping criterion has been reached. Either H1 in one of the tests or\n",
    "# H0 in all three tests\n",
    "if ((bf_main_surprise_H1 >= bf_thresh) or (bf_surprise_q_type_H1 >= bf_thresh) or \\\n",
    "    (bf_3way_interaction_H1 >= bf_thresh) or ((bf_main_surprise_H0 >= bf_thresh) and \\\n",
    "    (bf_surprise_q_type_H0 >= bf_thresh) and (bf_3way_interaction_H0 >= bf_thresh))):\n",
    "    stop_criterion_reached = True\n",
    "else:\n",
    "    stop_criterion_reached = False\n",
    "\n",
    "\n",
    "# Prints results of calculations, first verifying the chosen models are the correct ones\n",
    "if (not models_validated):\n",
    "    print('The ANOVA models chosen for the calculations do not match the expected ones.' + \\\n",
    "         'This may be due to different platforms/versions of the BayesFactor package.' + \\\n",
    "          'View cell above to identify the correct models from the bf variable (that includes)' + \\\n",
    "          'all models in the anova')\n",
    "else:\n",
    "    print('Evidence in favour of main effect of surprise (H1): %.3g' % (bf_main_surprise_H1))\n",
    "    print('Evidence against main effect of surprise (H0): %.3g'% (bf_main_surprise_H0))\n",
    "    print('Evidence in favour of q-type/surprise interaction (H1): %.3g'% (bf_surprise_q_type_H1))\n",
    "    print('Evidence against q-type/surprise interaction (H0): %.3g'% (bf_surprise_q_type_H0))\n",
    "    print('Evidence in favour of 3-way interaction (H1): %.3g'% (bf_3way_interaction_H1))\n",
    "    print('Evidence against 3-way interaction (H0): %.3g'% (bf_3way_interaction_H0))\n",
    "    \n",
    "    if (stop_criterion_reached):\n",
    "        print('Stop experiment - stopping criterion reached')\n",
    "    elif (batch_num==max_n_batch):\n",
    "        print('Stop experiment - stopping criterion not yet reached but max batches has')\n",
    "    else:\n",
    "        print('Continue experiment - stopping criterion not yet reached')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory performance\n",
    "Calculates average memory performance in each condition for reporting. Calculates both hit rate and Pr. Pr results are presented in Table 1 of the main text and hit rates in Supplementary Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates hit rates\n",
    "full_subj_avg_df['hit_rate'] = full_subj_avg_df['n_hit']/(full_subj_avg_df['n_hit']+full_subj_avg_df['n_miss'])\n",
    "full_subj_avg_df['HC_hit_rate'] = full_subj_avg_df['n_HC_hit']/(full_subj_avg_df['n_HC_hit']+\\\n",
    "        full_subj_avg_df['n_HC_miss'])\n",
    "\n",
    "# Calculates overall mean performance (and standard error) for target questions, divided by group and surprise\n",
    "T_mean = full_subj_avg_df[(~full_subj_avg_df['exc_subj'])&(full_subj_avg_df['q_type']=='T')]\\\n",
    "                .groupby(['group', 'surprise_type'])['hit_rate', 'HC_hit_rate', 'Pr', 'HC_Pr']\\\n",
    "                .mean().reset_index()   \n",
    "T_sem = full_subj_avg_df[(~full_subj_avg_df['exc_subj'])&(full_subj_avg_df['q_type']=='T')]\\\n",
    "                .groupby(['group', 'surprise_type'])['hit_rate', 'HC_hit_rate', 'Pr', 'HC_Pr']\\\n",
    "                .sem().reset_index()  \n",
    "\n",
    "# Calculates overall mean performance (and standard error) over non-target questions, divided only by group\n",
    "nonT_mean = full_subj_avg_df[(~full_subj_avg_df['exc_subj'])&(full_subj_avg_df['q_type']!='T')]\\\n",
    "                .groupby(['group'])['hit_rate', 'HC_hit_rate', 'Pr', 'HC_Pr'].mean().reset_index()   \n",
    "nonT_sem = full_subj_avg_df[(~full_subj_avg_df['exc_subj'])&(full_subj_avg_df['q_type']!='T')]\\\n",
    "                .groupby(['group'])['hit_rate', 'HC_hit_rate', 'Pr', 'HC_Pr'].sem().reset_index()  \n",
    "\n",
    "# Calculates mean and standard error for each action type X surprise X group ()\n",
    "performance_mean = full_subj_avg_df[~full_subj_avg_df['exc_subj']] \\\n",
    "                        .groupby(['group', 'q_type', 'surprise_type'])['hit_rate', 'HC_hit_rate', \\\n",
    "                        'Pr', 'HC_Pr'].mean().reset_index()\n",
    "performance_sem = full_subj_avg_df[~full_subj_avg_df['exc_subj']] \\\n",
    "                        .groupby(['group', 'q_type', 'surprise_type'])['hit_rate', 'HC_hit_rate', \\\n",
    "                        'Pr', 'HC_Pr'].sem().reset_index()\n",
    "                        \n",
    "\n",
    "print('mean performance - target questions:')\n",
    "print(T_mean)\n",
    "print('\\nperformance sem - target questions:')\n",
    "print(T_sem)\n",
    "print('\\nmean performance - all non-target questions:')\n",
    "print(nonT_mean)\n",
    "print('\\nperformance sem - all non-target questions:')\n",
    "print(nonT_sem)\n",
    "print('\\nmean performance - q-typeXsurpriseXgroup:')\n",
    "print(performance_mean)\n",
    "print('\\nperformance sem - q-typeXsurpriseXgroup:')\n",
    "print(performance_sem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primary analyses\n",
    "\n",
    "The analyses described in the 'Retroactive and proactive effects of surprise' section of the manuscript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proactive ANOVA (R)\n",
    "\n",
    "Runs a 3-way ANOVA for proactive effects (the retroactive ANOVA has already been calculated for the stopping criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i full_subj_avg_df\n",
    "\n",
    "# Sets seed to arbitrary value at the beginning of each R cell, for reproducibility\n",
    "set.seed(26)\n",
    "\n",
    "# Turns the subj column into a factor - will be converted to numeric by default\n",
    "# Also sets the others to be factors (though they're already strings), since something\n",
    "# in the conversion in the notebook works differently and at least one isn't recognised \n",
    "# as a factor otherwise\n",
    "full_subj_avg_df[,'subj'] <- as.factor(full_subj_avg_df[,'subj'])\n",
    "full_subj_avg_df[,'q_type'] <- as.factor(full_subj_avg_df[,'q_type'])\n",
    "full_subj_avg_df[,'surprise_type'] <- as.factor(full_subj_avg_df[,'surprise_type'])\n",
    "full_subj_avg_df[,'group'] <- as.factor(full_subj_avg_df[,'group'])\n",
    "\n",
    "# Gets the subset of data for the proactive test\n",
    "proactive_data <- subset(full_subj_avg_df, (q_type=='preT' | q_type=='postT') & exc_subj=='FALSE')\n",
    "\n",
    "# Runs a 3-way anova for the proactive effects, and all subsequent comparisons that were run for the \n",
    "# retroactive effects\n",
    "pro_bf = anovaBF(Pr ~ q_type*surprise_type*group + subj, data=proactive_data, \n",
    "                 whichRandom='subj')\n",
    "\n",
    "# Extracts BF in favour of the alternative (H1) for each of the tests described above\n",
    "pro_bf_main_surprise_H1 = as.numeric(as.vector(pro_bf[9]/pro_bf[4]))\n",
    "pro_bf_surprise_q_type_H1 = as.numeric(as.vector(pro_bf[17]/pro_bf[12]))\n",
    "pro_bf_3way_interaction_H1 = as.numeric(as.vector(pro_bf[18]/pro_bf[17]))\n",
    "\n",
    "# Makes sure the correct models were taken from the full anova (in case the order can change)\n",
    "pro_validated <- TRUE\n",
    "pro_validated <- pro_validated & setequal(unlist(strsplit(names(as.vector(pro_bf[4])), ' + ', TRUE)), \n",
    "                        c('group','q_type','group:q_type','subj'))\n",
    "pro_validated <- pro_validated & setequal(unlist(strsplit(names(as.vector(pro_bf[9])), ' + ', TRUE)), \n",
    "                        c('group','q_type','group:q_type', 'surprise_type','subj'))\n",
    "pro_validated <- pro_validated & setequal(unlist(strsplit(names(as.vector(pro_bf[12])), ' + ', TRUE)),\n",
    "                        c('group','q_type','surprise_type', 'group:q_type','group:surprise_type','subj'))\n",
    "pro_validated <- pro_validated & setequal(unlist(strsplit(names(as.vector(pro_bf[17])), ' + ', TRUE)),\n",
    "                        c('group','q_type','surprise_type', 'group:q_type','group:surprise_type',\n",
    "                          'q_type:surprise_type','subj'))\n",
    "pro_validated <- pro_validated & setequal(unlist(strsplit(names(as.vector(pro_bf[18])), ' + ', TRUE)),\n",
    "                        c('group','q_type','surprise_type', 'group:q_type','group:surprise_type',\n",
    "                          'q_type:surprise_type','group:q_type:surprise_type', 'subj'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retroactive and proactive planned t-tests (R)\n",
    "\n",
    "Runs t-tests for the effect of surprise (Pr Surprise - Pr Neutral) for each question-type:\n",
    "- within each group\n",
    "- collapsing across groups\n",
    "- direct comparison between the groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i full_diff_df\n",
    "\n",
    "# Sets seed to arbitrary value at the beginning of each R cell, for reproducibility\n",
    "set.seed(26)\n",
    "\n",
    "# Turns the subj column into a factor - will be converted to numeric by default\n",
    "# Also sets the others to be factors (though they're already strings), since something\n",
    "# in the conversion in the notebook works differently and at least one isn't recognised \n",
    "# as a factor otherwise\n",
    "full_diff_df[,'subj'] <- as.factor(full_diff_df[,'subj'])\n",
    "full_diff_df[,'q_type'] <- as.factor(full_diff_df[,'q_type'])\n",
    "full_diff_df[,'group'] <- as.factor(full_diff_df[,'group'])\n",
    "\n",
    "# Slices the data to have the relevant subsets for each test\n",
    "preS_I_diff <- subset(full_diff_df, q_type=='preS' & group=='I' & exc_subj=='FALSE')\n",
    "preT_I_diff <- subset(full_diff_df, q_type=='preT' & group=='I' & exc_subj=='FALSE')\n",
    "postT_I_diff <- subset(full_diff_df, q_type=='postT' & group=='I' & exc_subj=='FALSE')\n",
    "preS_D_diff <- subset(full_diff_df, q_type=='preS' & group=='D' & exc_subj=='FALSE')\n",
    "preT_D_diff <- subset(full_diff_df, q_type=='preT' & group=='D' & exc_subj=='FALSE')\n",
    "postT_D_diff <- subset(full_diff_df, q_type=='postT' & group=='D' & exc_subj=='FALSE')\n",
    "preS_comb_diff <- subset(full_diff_df, q_type=='preS' & exc_subj=='FALSE')\n",
    "preT_comb_diff <- subset(full_diff_df, q_type=='preT' & exc_subj=='FALSE')\n",
    "postT_comb_diff <- subset(full_diff_df, q_type=='postT' & exc_subj=='FALSE')\n",
    "\n",
    "\n",
    "# Runs the planned t-tests, looking at surprise effects within each question type\n",
    "preS_I_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(preS_I_diff['Pr_diff'])))))\n",
    "preT_I_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(preT_I_diff['Pr_diff'])))))\n",
    "postT_I_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(postT_I_diff['Pr_diff'])))))\n",
    "preS_D_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(preS_D_diff['Pr_diff'])))))\n",
    "preT_D_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(preT_D_diff['Pr_diff'])))))\n",
    "postT_D_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(postT_D_diff['Pr_diff'])))))\n",
    "preS_comb_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(preS_comb_diff['Pr_diff'])))))\n",
    "preT_comb_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(preT_comb_diff['Pr_diff'])))))\n",
    "postT_comb_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(postT_comb_diff['Pr_diff'])))))\n",
    "preS_comp_bf_H1 <- as.numeric(as.vector(ttestBF(x=as.numeric(unlist(preS_I_diff['Pr_diff'])), \n",
    "                                                  y=as.numeric(unlist(preS_D_diff['Pr_diff'])))))\n",
    "preT_comp_bf_H1 <- as.numeric(as.vector(ttestBF(x=as.numeric(unlist(preT_I_diff['Pr_diff'])), \n",
    "                                                  y=as.numeric(unlist(preT_D_diff['Pr_diff'])))))\n",
    "postT_comp_bf_H1 <- as.numeric(as.vector(ttestBF(x=as.numeric(unlist(postT_I_diff['Pr_diff'])), \n",
    "                                                  y=as.numeric(unlist(postT_D_diff['Pr_diff'])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary analyses - displays results\n",
    "Display evidence in favour of H1 (BF10) / in favour of the null (BF01) for each test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls the relevant variables from the R code and converts to python native data types\n",
    "\n",
    "%Rpull pro_validated pro_bf_main_surprise_H1 pro_bf_surprise_q_type_H1 pro_bf_3way_interaction_H1 \\\n",
    "        preS_I_bf_H1 preT_I_bf_H1 postT_I_bf_H1 preS_D_bf_H1 preT_D_bf_H1 postT_D_bf_H1 preS_comb_bf_H1 \\\n",
    "        preT_comb_bf_H1 postT_comb_bf_H1 preS_comp_bf_H1 preT_comp_bf_H1 postT_comp_bf_H1\n",
    "\n",
    "pro_validated = bool(np.asarray(pro_validated)[0])\n",
    "pro_bf_main_surprise_H1 = np.asarray(pro_bf_main_surprise_H1)[0]\n",
    "pro_bf_surprise_q_type_H1 = np.asarray(pro_bf_surprise_q_type_H1)[0]\n",
    "pro_bf_3way_interaction_H1 = np.asarray(pro_bf_3way_interaction_H1)[0]\n",
    "preS_I_bf_H1 = np.asarray(preS_I_bf_H1)[0]\n",
    "preT_I_bf_H1 = np.asarray(preT_I_bf_H1)[0]\n",
    "postT_I_bf_H1 = np.asarray(postT_I_bf_H1)[0]\n",
    "preS_D_bf_H1 = np.asarray(preS_D_bf_H1)[0]\n",
    "preT_D_bf_H1 = np.asarray(preT_D_bf_H1)[0]\n",
    "postT_D_bf_H1 = np.asarray(postT_D_bf_H1)[0]\n",
    "preS_comb_bf_H1 = np.asarray(preS_comb_bf_H1)[0]\n",
    "preT_comb_bf_H1 = np.asarray(preT_comb_bf_H1)[0]\n",
    "postT_comb_bf_H1 = np.asarray(postT_comb_bf_H1)[0]\n",
    "preS_comp_bf_H1 = np.asarray(preS_comp_bf_H1)[0]\n",
    "preT_comp_bf_H1 = np.asarray(preT_comp_bf_H1)[0]\n",
    "postT_comp_bf_H1 = np.asarray(postT_comp_bf_H1)[0]\n",
    "\n",
    "\n",
    "# Prints results of calculations, first verifying the chosen models are the correct ones\n",
    "if (not pro_validated):\n",
    "    print('The ANOVA models chosen for the calculations do not match the expected ones.' + \\\n",
    "         'This may be due to different platforms/versions of the BayesFactor package.' + \\\n",
    "          'View cell above to identify the correct models from the bf variable (that includes)' + \\\n",
    "          'all models in the anova')\n",
    "else:\n",
    "    print ('Results of proactive ANOVA:')\n",
    "    print('   Evidence for/against main effect of surprise: %.3g/%.3g'% \\\n",
    "          (pro_bf_main_surprise_H1,1/pro_bf_main_surprise_H1))\n",
    "    print('   Evidence for/against q-type/surprise interaction: %.3g/%.3g'% \\\n",
    "          (pro_bf_surprise_q_type_H1, 1/pro_bf_surprise_q_type_H1))\n",
    "    print('   Evidence for/against 3-way interaction: %.3g/%.3g'% \\\n",
    "          (pro_bf_3way_interaction_H1, 1/pro_bf_3way_interaction_H1))\n",
    "    print('\\nResults of planned t-tests:')\n",
    "    print('   Evidence for/against surprise effect in preS (immediate group): '\\\n",
    "              + '%.3g/%.3g'% (preS_I_bf_H1, 1/preS_I_bf_H1))\n",
    "    print('   Evidence for/against surprise effect in preT (immediate group): '\\\n",
    "              + '%.3g/%.3g'% (preT_I_bf_H1, 1/preT_I_bf_H1))\n",
    "    print('   Evidence for/against surprise effect in postT (immediate group): '\\\n",
    "              + '%.3g/%.3g'% (postT_I_bf_H1, 1/postT_I_bf_H1))\n",
    "    print('   Evidence for/against surprise effect in preS (delay group): '\\\n",
    "              + '%.3g/%.3g'% (preS_D_bf_H1, 1/preS_D_bf_H1))\n",
    "    print('   Evidence for/against surprise effect in preT (delay group): '\\\n",
    "              + '%.3g/%.3g'% (preT_D_bf_H1, 1/preT_D_bf_H1))\n",
    "    print('   Evidence for/against surprise effect in postT (delay group): '\\\n",
    "              + '%.3g/%.3g'% (postT_D_bf_H1, 1/postT_D_bf_H1))\n",
    "    print('   Evidence for/against surprise effect in preS (collapsed): ' + \\\n",
    "              '%.3g/%.3g'% (preS_comb_bf_H1, 1/preS_comb_bf_H1))\n",
    "    print('   Evidence for/against surprise effect in preT (collapsed): ' + \\\n",
    "              '%.3g/%.3g'% (preT_comb_bf_H1, 1/preT_comb_bf_H1))\n",
    "    print('   Evidence for/against surprise effect in postT (collapsed): ' + \\\n",
    "              '%.3g/%.3g'% (postT_comb_bf_H1, 1/postT_comb_bf_H1))\n",
    "    print('   Evidence for/against surprise effect in preS (comparison between groups): ' + \\\n",
    "              '%.3g/%.3g'% (preS_comp_bf_H1, 1/preS_comp_bf_H1))\n",
    "    print('   Evidence for/against surprise effect in preT (comparison between groups): ' + \\\n",
    "              '%.3g/%.3g'% (preT_comp_bf_H1, 1/preT_comp_bf_H1))\n",
    "    print('   Evidence for/against surprise effect in postT (comparison between groups): ' + \\\n",
    "              '%.3g/%.3g'% (postT_comp_bf_H1, 1/postT_comp_bf_H1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results\n",
    "\n",
    "Plots the surprise effect per question type, separately for each group, so the results of the analyses can be interpreted. Also creates and displays a dataframe with the mean, standard deviation and standard error for each question type X group. Figure 2 of main text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the surprise effect for each group, as well as combined across groups\n",
    "\n",
    "for group in (['immediate', 'delay']):\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h), dpi=plot_dpi)\n",
    "    ax.axhline(0, color='k')\n",
    "    sns.violinplot(x='q_type', y='Pr_diff',data=full_diff_df[(full_diff_df['group']==group[0].upper())&\\\n",
    "                        (full_diff_df['exc_subj']==False)], linewidth=2.5, ax=ax, \n",
    "                           order=['preS', 'preT', 'T', 'postT'], palette='Set2', saturation=1)\n",
    "    fig.suptitle(group.capitalize(), fontsize=title_font)\n",
    "    plt.xlabel('Action Type', fontsize=label_font)\n",
    "    plt.ylabel('Surprise effect (Pr-S - Pr-N)', fontsize=label_font)\n",
    "    plt.yticks(np.arange(-1,1,0.5), fontsize=tick_font)\n",
    "    plt.xticks(fontsize=tick_font)\n",
    "    if (save_figs):\n",
    "        fig.savefig(f'{fig_dir}surprise_effect_{group}.eps')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(fig_w, fig_h), dpi=plot_dpi)\n",
    "ax.axhline(0, color='k')\n",
    "sns.violinplot(x='q_type', y='Pr_diff',data=full_diff_df[(full_diff_df['exc_subj']==False)],\n",
    "               linewidth=2.5, ax=ax, order=['preS', 'preT', 'T', 'postT'], palette='Set2', saturation=1)\n",
    "fig.suptitle('Collapsed', fontsize=title_font)\n",
    "plt.xlabel('Action Type', fontsize=label_font)\n",
    "plt.ylabel('Surprise effect (Pr-S - Pr-N)', fontsize=label_font)\n",
    "plt.yticks(np.arange(-1,1,0.5), fontsize=tick_font)\n",
    "plt.xticks(fontsize=tick_font)\n",
    "if (save_figs):\n",
    "    fig.savefig(f'{fig_dir}surprise_effect_collapsed.eps')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots for Supplementary Figure 3 - difference in surprise effect between action types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the difference in the surprise effect (preT-preS and postT-preT) for each group, as well as combined across groups\n",
    "\n",
    "# First creates a version of the dataframe with the preT-preS and postT-preT surprise differences\n",
    "effect_diff_df = full_diff_df.pivot_table(index=['subj', 'group', 'exc_subj'], \n",
    "                                           columns='q_type', values=['Pr_diff', 'HC_Pr_diff'])\n",
    "\n",
    "effect_diff_df.columns = list(map(\"_\".join, effect_diff_df.columns))\n",
    "effect_diff_df.reset_index(inplace=True)\n",
    "effect_diff_df['preT_preS_Pr_diff'] = effect_diff_df['Pr_diff_preT']-effect_diff_df['Pr_diff_preS']\n",
    "effect_diff_df['preT_preS_HC_Pr_diff'] = effect_diff_df['HC_Pr_diff_preT']-effect_diff_df['HC_Pr_diff_preS']\n",
    "effect_diff_df['postT_preT_Pr_diff'] = effect_diff_df['Pr_diff_postT']-effect_diff_df['Pr_diff_preT']\n",
    "effect_diff_df['postT_preT_HC_Pr_diff'] = effect_diff_df['HC_Pr_diff_postT']-effect_diff_df['HC_Pr_diff_preT']\n",
    "\n",
    "# Plots the preT-preS difference alongside the postT-preT difference\n",
    "for group in (['immediate', 'delay']):\n",
    "    fig, ax = plt.subplots(figsize=(fig_w,fig_h), dpi=plot_dpi)\n",
    "    ax.axhline(0, color='k')\n",
    "    sns.violinplot(data=effect_diff_df[(effect_diff_df['group']==group[0].upper())&\\\n",
    "                        (effect_diff_df['exc_subj']==False)][['preT_preS_Pr_diff', 'postT_preT_Pr_diff']], linewidth=2.5, ax=ax, \n",
    "                           order=['preT_preS_Pr_diff', 'postT_preT_Pr_diff'], palette=[(0.16, 0.52, 0.77),(0.88, 0.51, 0.17)], saturation=1)\n",
    "    fig.suptitle(group.capitalize(), fontsize=title_font)\n",
    "    plt.xlabel('Action Type', fontsize=label_font)\n",
    "    plt.ylabel('Surprise effect difference', fontsize=label_font)\n",
    "    plt.yticks(np.arange(-1,1,0.5), fontsize=tick_font)\n",
    "    plt.xticks(fontsize=tick_font)\n",
    "    if (save_figs):\n",
    "        fig.savefig(f'{fig_dir}surprise_effect_diff_{group}.eps')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(fig_w,fig_h), dpi=plot_dpi)\n",
    "ax.axhline(0, color='k')\n",
    "sns.violinplot(data=effect_diff_df[(effect_diff_df['exc_subj']==False)][['preT_preS_Pr_diff', 'postT_preT_Pr_diff']],\n",
    "               linewidth=2.5, ax=ax, order=['preT_preS_Pr_diff', 'postT_preT_Pr_diff'], palette=[(0.16, 0.52, 0.77),(0.88, 0.51, 0.17)], saturation=1)\n",
    "fig.suptitle('Collapsed', fontsize=title_font)\n",
    "plt.xlabel('Question Type', fontsize=label_font)\n",
    "plt.ylabel('Action effect difference', fontsize=label_font)\n",
    "plt.yticks(np.arange(-1,1,0.5), fontsize=tick_font)\n",
    "plt.xticks(fontsize=tick_font)\n",
    "if (save_figs):\n",
    "    fig.savefig(f'{fig_dir}surprise_effect_diff_collapsed.eps')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displays mean, sd, and se in each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.3g}'.format\n",
    "\n",
    "# Creates dataframe wth mean, sd and se of each condition and displays\n",
    "mean_effect_df = full_diff_df[(full_diff_df['exc_subj']==False)].groupby(['q_type', 'group'])['Pr_diff'].\\\n",
    "                    agg(['mean', 'std', 'sem'])\n",
    "display(mean_effect_df)\n",
    "\n",
    "# Creates dataframe wth mean, sd and se of preT-preS and postT-preT differences, and displays\n",
    "\n",
    "mean_diff_effect_df = effect_diff_df[(effect_diff_df['exc_subj']==False)].groupby(['group'])[['preT_preS_Pr_diff', 'preT_preS_HC_Pr_diff', 'postT_preT_Pr_diff',\n",
    "                            'postT_preT_HC_Pr_diff']].agg(['mean', 'std', 'sem'])\n",
    "\n",
    "display(mean_diff_effect_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculates effect size\n",
    "\n",
    "Calculates the effect size for the postT effects - both for postT itself and for the difference between preT and postT (the interaction). Does this once for the delay group and once for both groups combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates effect size for the postT effects\n",
    "\n",
    "# postT in delay group\n",
    "postT_delay_subset = full_diff_df[(full_diff_df['exc_subj']==False)&(full_diff_df['q_type']=='postT')&\\\n",
    "                                   (full_diff_df['group']=='D')]\n",
    "eff_size_postT_delay = np.mean(postT_delay_subset['Pr_diff'])/np.std(postT_delay_subset['Pr_diff'])\n",
    "\n",
    "# postT collapsed across groups\n",
    "postT_comb_subset = full_diff_df[(full_diff_df['exc_subj']==False)&(full_diff_df['q_type']=='postT')]\n",
    "eff_size_postT_comb = np.mean(postT_comb_subset['Pr_diff'])/np.std(postT_comb_subset['Pr_diff'])\n",
    "\n",
    "# Creates a version of the dataframe with the surprise effect (Pr-S - Pr-N) of postT subtracted from\n",
    "# the effect of preT to estimate the effect size of the difference\n",
    "preT_diff_df = full_diff_df[full_diff_df['q_type']=='preT'].drop(\\\n",
    "                columns=['Pr-S', 'HC_Pr-S', 'Pr-N', 'HC_Pr-N', 'q_type']).\\\n",
    "                rename({'Pr_diff': 'Pr_diff_preT', 'HC_Pr_diff': 'HC_Pr_diff_preT'}, axis=1)\n",
    "postT_diff_df = full_diff_df[full_diff_df['q_type']=='postT'].drop(\\\n",
    "                columns=['Pr-S', 'HC_Pr-S', 'Pr-N', 'HC_Pr-N', 'q_type']).\\\n",
    "                rename({'Pr_diff': 'Pr_diff_postT', 'HC_Pr_diff': 'HC_Pr_diff_postT'}, axis=1)\n",
    "pro_diff_df = preT_diff_df.merge(postT_diff_df, validate='one_to_one')\n",
    "pro_diff_df['Pr_diff'] = pro_diff_df['Pr_diff_preT']-pro_diff_df['HC_Pr_diff_postT']\n",
    "pro_diff_df['HC_Pr_diff'] = pro_diff_df['HC_Pr_diff_preT']-pro_diff_df['HC_Pr_diff_postT']\n",
    "\n",
    "# preT vs. postT in delay group\n",
    "pro_diff_delay = pro_diff_df[(pro_diff_df['exc_subj']==False)&(pro_diff_df['group']=='D')]\n",
    "eff_size_pro_diff_delay = np.mean(pro_diff_delay['Pr_diff'])/np.std(pro_diff_delay['Pr_diff'])\n",
    "\n",
    "# preT vs. postT collapsed across groups\n",
    "pro_diff_comb = pro_diff_df[(pro_diff_df['exc_subj']==False)]\n",
    "eff_size_pro_diff_comb = np.mean(pro_diff_comb['Pr_diff'])/np.std(pro_diff_comb['Pr_diff'])\n",
    "\n",
    "print('Proactive effect sizes:')\n",
    "print('   postT effect (t-test) - delay group: %.3g'% eff_size_postT_delay)\n",
    "print('   postT effect (t-test) - combined: %.3g'% eff_size_postT_comb)\n",
    "print('   preT-postT diff (interaction) - delay group: %.3g'% eff_size_pro_diff_delay)\n",
    "print('   preT-postT diff (interaction) - combined: %.3g'% eff_size_pro_diff_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondary analyses - high-confidence Pr\n",
    "\n",
    "Runs all of the analyses a second time, looking at high-confidence Pr (Hit-FA for answers with confidence of 2) instead of regular Pr (confidence of 1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary analyses - HC Pr - retroactive and proactive ANOVAs (R)\n",
    "\n",
    "Same as the retroactive and proactive analyses above. Skips model validation since model order should be identical regardless of the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i full_subj_avg_df\n",
    "\n",
    "# Sets seed to arbitrary value at the beginning of each R cell, for reproducibility\n",
    "set.seed(26)\n",
    "\n",
    "# Turns the subj column into a factor - will be converted to numeric by default\n",
    "# Also sets the others to be factors (though they're already strings), since something\n",
    "# in the conversion in the notebook works differently and at least one isn't recognised \n",
    "# as a factor otherwise\n",
    "full_subj_avg_df[,'subj'] <- as.factor(full_subj_avg_df[,'subj'])\n",
    "full_subj_avg_df[,'q_type'] <- as.factor(full_subj_avg_df[,'q_type'])\n",
    "full_subj_avg_df[,'surprise_type'] <- as.factor(full_subj_avg_df[,'surprise_type'])\n",
    "full_subj_avg_df[,'group'] <- as.factor(full_subj_avg_df[,'group'])\n",
    "\n",
    "# Takes only preS/preT questions for retroactive analysis, preT/postT for proactive, \n",
    "# and removes all excluded subjects in both. These will be identical to those calculated above,\n",
    "# but duplicates code so each analysis can be run independently\n",
    "retroactive_data <- subset(full_subj_avg_df, (q_type=='preS' | q_type=='preT') & exc_subj=='FALSE')\n",
    "proactive_data <- subset(full_subj_avg_df, (q_type=='preT' | q_type=='postT') & exc_subj=='FALSE')\n",
    "\n",
    "# Runs 3-way anovas with question-type, surprise and group, here using HC Pr\n",
    "retro_HC_bf = anovaBF(HC_Pr ~ q_type*surprise_type*group + subj, \n",
    "                      data=retroactive_data, whichRandom='subj')\n",
    "pro_HC_bf = anovaBF(HC_Pr ~ q_type*surprise_type*group + subj, \n",
    "                    data=proactive_data, whichRandom='subj')\n",
    "\n",
    "# Extracts BF in favour of the alternative (H1) for each of the criteria\n",
    "retro_HC_bf_main_surprise_H1 = as.numeric(as.vector(retro_HC_bf[9]/retro_HC_bf[4]))\n",
    "retro_HC_bf_surprise_q_type_H1 = as.numeric(as.vector(retro_HC_bf[17]/retro_HC_bf[12]))\n",
    "retro_HC_bf_3way_interaction_H1 = as.numeric(as.vector(retro_HC_bf[18]/retro_HC_bf[17]))\n",
    "pro_HC_bf_main_surprise_H1 = as.numeric(as.vector(pro_HC_bf[9]/pro_HC_bf[4]))\n",
    "pro_HC_bf_surprise_q_type_H1 = as.numeric(as.vector(pro_HC_bf[17]/pro_HC_bf[12]))\n",
    "pro_HC_bf_3way_interaction_H1 = as.numeric(as.vector(pro_HC_bf[18]/pro_HC_bf[17])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary analyses - HC Pr - retroactive and proactive ANOVAs - display results\n",
    "\n",
    "Displays results of the high-confidence anovas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls the relevant variables from the R code and converts to python native data types\n",
    "\n",
    "%Rpull retro_HC_bf_main_surprise_H1 retro_HC_bf_surprise_q_type_H1 retro_HC_bf_3way_interaction_H1 \\\n",
    "pro_HC_bf_main_surprise_H1 pro_HC_bf_surprise_q_type_H1 pro_HC_bf_3way_interaction_H1\n",
    "\n",
    "retro_HC_bf_main_surprise_H1 = np.asarray(retro_HC_bf_main_surprise_H1)[0]\n",
    "retro_HC_bf_surprise_q_type_H1 = np.asarray(retro_HC_bf_surprise_q_type_H1)[0]\n",
    "retro_HC_bf_3way_interaction_H1 = np.asarray(retro_HC_bf_3way_interaction_H1)[0]\n",
    "pro_HC_bf_main_surprise_H1 = np.asarray(pro_HC_bf_main_surprise_H1)[0]\n",
    "pro_HC_bf_surprise_q_type_H1 = np.asarray(pro_HC_bf_surprise_q_type_H1)[0]\n",
    "pro_HC_bf_3way_interaction_H1 = np.asarray(pro_HC_bf_3way_interaction_H1)[0]\n",
    "retro_HC_bf_main_surprise_H0 = 1/retro_HC_bf_main_surprise_H1\n",
    "retro_HC_bf_surprise_q_type_H0 = 1/retro_HC_bf_surprise_q_type_H1\n",
    "retro_HC_bf_3way_interaction_H0 = 1/retro_HC_bf_3way_interaction_H1\n",
    "pro_HC_bf_main_surprise_H0 = 1/pro_HC_bf_main_surprise_H1\n",
    "pro_HC_bf_surprise_q_type_H0 = 1/pro_HC_bf_surprise_q_type_H1\n",
    "pro_HC_bf_3way_interaction_H0 = 1/pro_HC_bf_3way_interaction_H1\n",
    "\n",
    "\n",
    "print ('Results of retroactive HC-Pr ANOVA:')\n",
    "print('   Evidence in favour of main effect of surprise (H1): %.3g'% retro_HC_bf_main_surprise_H1)\n",
    "print('   Evidence against main effect of surprise (H0): %.3g'% retro_HC_bf_main_surprise_H0)\n",
    "print('   Evidence in favour of q-type/surprise interaction (H1): %.3g'% (retro_HC_bf_surprise_q_type_H1))\n",
    "print('   Evidence against q-type/surprise interaction (H0): %.3g'% (retro_HC_bf_surprise_q_type_H0))\n",
    "print('   Evidence in favour of 3-way interaction (H1): %.3g'% (retro_HC_bf_3way_interaction_H1))\n",
    "print('   Evidence against 3-way interaction (H0): %.3g'% (retro_HC_bf_3way_interaction_H0))\n",
    "\n",
    "print ('\\nResults of proactive HC-Pr ANOVA:')\n",
    "print('   Evidence in favour of main effect of surprise (H1): %.3g'% pro_HC_bf_main_surprise_H1)\n",
    "print('   Evidence against main effect of surprise (H0): %.3g'% pro_HC_bf_main_surprise_H0)\n",
    "print('   Evidence in favour of q-type/surprise interaction (H1): %.3g'% (pro_HC_bf_surprise_q_type_H1))\n",
    "print('   Evidence against q-type/surprise interaction (H0): %.3g'% (pro_HC_bf_surprise_q_type_H0))\n",
    "print('   Evidence in favour of 3-way interaction (H1): %.3g'% (pro_HC_bf_3way_interaction_H1))\n",
    "print('   Evidence against 3-way interaction (H0): %.3g'% (pro_HC_bf_3way_interaction_H0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary analyses - HC Pr - planned t-tests (R)\n",
    "\n",
    "Same as the retroactive and proactive planned t-tests above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i full_diff_df\n",
    "\n",
    "# Sets seed to arbitrary value at the beginning of each R cell, for reproducibility\n",
    "set.seed(26)\n",
    "\n",
    "# Turns the subj column into a factor - will be converted to numeric by default\n",
    "# Also sets the others to be factors (though they're already strings), since something\n",
    "# in the conversion in the notebook works differently and at least one isn't recognised \n",
    "# as a factor otherwise\n",
    "full_diff_df[,'subj'] <- as.factor(full_diff_df[,'subj'])\n",
    "full_diff_df[,'q_type'] <- as.factor(full_diff_df[,'q_type'])\n",
    "full_diff_df[,'group'] <- as.factor(full_diff_df[,'group'])\n",
    "\n",
    "# Slices the data to have the relevant subsets for each test\n",
    "preS_I_diff <- subset(full_diff_df, q_type=='preS' & group=='I' & exc_subj=='FALSE')\n",
    "preT_I_diff <- subset(full_diff_df, q_type=='preT' & group=='I' & exc_subj=='FALSE')\n",
    "postT_I_diff <- subset(full_diff_df, q_type=='postT' & group=='I' & exc_subj=='FALSE')\n",
    "preS_D_diff <- subset(full_diff_df, q_type=='preS' & group=='D' & exc_subj=='FALSE')\n",
    "preT_D_diff <- subset(full_diff_df, q_type=='preT' & group=='D' & exc_subj=='FALSE')\n",
    "postT_D_diff <- subset(full_diff_df, q_type=='postT' & group=='D' & exc_subj=='FALSE')\n",
    "preS_comb_diff <- subset(full_diff_df, q_type=='preS' & exc_subj=='FALSE')\n",
    "preT_comb_diff <- subset(full_diff_df, q_type=='preT' & exc_subj=='FALSE')\n",
    "postT_comb_diff <- subset(full_diff_df, q_type=='postT' & exc_subj=='FALSE')\n",
    "\n",
    "\n",
    "# Runs the planned t-tests, looking at surprise effects within each question type. Here takes HC_Pr_diff\n",
    "# instead of Pr_diff\n",
    "HC_preS_I_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(preS_I_diff['HC_Pr_diff'])))))\n",
    "HC_preT_I_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(preT_I_diff['HC_Pr_diff'])))))\n",
    "HC_postT_I_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(postT_I_diff['HC_Pr_diff'])))))\n",
    "HC_preS_D_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(preS_D_diff['HC_Pr_diff'])))))\n",
    "HC_preT_D_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(preT_D_diff['HC_Pr_diff'])))))\n",
    "HC_postT_D_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(postT_D_diff['HC_Pr_diff'])))))\n",
    "HC_preS_comb_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(preS_comb_diff['HC_Pr_diff'])))))\n",
    "HC_preT_comb_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(preT_comb_diff['HC_Pr_diff'])))))\n",
    "HC_postT_comb_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(postT_comb_diff['HC_Pr_diff'])))))\n",
    "HC_preS_comp_bf_H1 <- as.numeric(as.vector(ttestBF(x=as.numeric(unlist(preS_I_diff['HC_Pr_diff'])), \n",
    "                                                  y=as.numeric(unlist(preS_D_diff['HC_Pr_diff'])))))\n",
    "HC_preT_comp_bf_H1 <- as.numeric(as.vector(ttestBF(x=as.numeric(unlist(preT_I_diff['HC_Pr_diff'])), \n",
    "                                                  y=as.numeric(unlist(preT_D_diff['HC_Pr_diff'])))))\n",
    "HC_postT_comp_bf_H1 <- as.numeric(as.vector(ttestBF(x=as.numeric(unlist(postT_I_diff['HC_Pr_diff'])), \n",
    "                                                  y=as.numeric(unlist(postT_D_diff['HC_Pr_diff'])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary analyses - HC Pr - planned t-tests - display results\n",
    "\n",
    "Displays only evidence in favour of H1 (BF10) / in favour of the null (BF01) for each test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls the relevant variables from the R code and converts to python native data types\n",
    "\n",
    "%Rpull HC_preS_I_bf_H1 HC_preT_I_bf_H1 HC_postT_I_bf_H1 HC_preS_D_bf_H1 HC_preT_D_bf_H1 HC_postT_D_bf_H1 \\\n",
    "HC_preS_comb_bf_H1 HC_preT_comb_bf_H1 HC_postT_comb_bf_H1 HC_preS_comp_bf_H1 HC_preT_comp_bf_H1 \\\n",
    "HC_postT_comp_bf_H1\n",
    "\n",
    "HC_preS_I_bf_H1 = np.asarray(HC_preS_I_bf_H1)[0]\n",
    "HC_preT_I_bf_H1 = np.asarray(HC_preT_I_bf_H1)[0]\n",
    "HC_postT_I_bf_H1 = np.asarray(HC_postT_I_bf_H1)[0]\n",
    "HC_preS_D_bf_H1 = np.asarray(HC_preS_D_bf_H1)[0]\n",
    "HC_preT_D_bf_H1 = np.asarray(HC_preT_D_bf_H1)[0]\n",
    "HC_postT_D_bf_H1 = np.asarray(HC_postT_D_bf_H1)[0]\n",
    "HC_preS_comb_bf_H1 = np.asarray(HC_preS_comb_bf_H1)[0]\n",
    "HC_preT_comb_bf_H1 = np.asarray(HC_preT_comb_bf_H1)[0]\n",
    "HC_postT_comb_bf_H1 = np.asarray(HC_postT_comb_bf_H1)[0]\n",
    "HC_preS_comp_bf_H1 = np.asarray(HC_preS_comp_bf_H1)[0]\n",
    "HC_preT_comp_bf_H1 = np.asarray(HC_preT_comp_bf_H1)[0]\n",
    "HC_postT_comp_bf_H1 = np.asarray(HC_postT_comp_bf_H1)[0]\n",
    "\n",
    "print('Evidence for/against surprise effect in preS (immediate group): '\\\n",
    "          + '%.3g/%.3g'% (HC_preS_I_bf_H1, 1/HC_preS_I_bf_H1))\n",
    "print('Evidence for/against surprise effect in preT (immediate group): '\\\n",
    "          + '%.3g/%.3g'% (HC_preT_I_bf_H1, 1/HC_preT_I_bf_H1))\n",
    "print('Evidence for/against surprise effect in postT (immediate group): '\\\n",
    "          + '%.3g/%.3g'% (HC_postT_I_bf_H1, 1/HC_postT_I_bf_H1))\n",
    "print('Evidence for/against surprise effect in preS (delay group): '\\\n",
    "          + '%.3g/%.3g'% (HC_preS_D_bf_H1, 1/HC_preS_D_bf_H1))\n",
    "print('Evidence for/against surprise effect in preT (delay group): '\\\n",
    "          + '%.3g/%.3g'% (HC_preT_D_bf_H1, 1/HC_preT_D_bf_H1))\n",
    "print('Evidence for/against surprise effect in postT (delay group): '\\\n",
    "          + '%.3g/%.3g'% (HC_postT_D_bf_H1, 1/HC_postT_D_bf_H1))\n",
    "print('Evidence for/against surprise effect in preS (collapsed): ' + \\\n",
    "          '%.3g/%.3g'% (HC_preS_comb_bf_H1, 1/HC_preS_comb_bf_H1))\n",
    "print('Evidence for/against surprise effect in preT (collapsed): ' + \\\n",
    "          '%.3g/%.3g'% (HC_preT_comb_bf_H1, 1/HC_preT_comb_bf_H1))\n",
    "print('Evidence for/against surprise effect in postT (collapsed): ' + \\\n",
    "          '%.3g/%.3g'% (HC_postT_comb_bf_H1, 1/HC_postT_comb_bf_H1))\n",
    "print('Evidence for/against surprise effect in preS (comparison between groups): ' + \\\n",
    "          '%.3g/%.3g'% (HC_preS_comp_bf_H1, 1/HC_preS_comp_bf_H1))\n",
    "print('Evidence for/against surprise effect in preT (comparison between groups): ' + \\\n",
    "          '%.3g/%.3g'% (HC_preT_comp_bf_H1, 1/HC_preT_comp_bf_H1))\n",
    "print('Evidence for/against surprise effect in postT (comparison between groups): ' + \\\n",
    "          '%.3g/%.3g'% (HC_postT_comp_bf_H1, 1/HC_postT_comp_bf_H1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary analyses - HC Pr - Plot results\n",
    "\n",
    "As above, plots the surprise effect per question type, separately for each group, so the results of the analyses can be interpreted. Also creates a dataframe with the mean, standard deviation and standard error for each question type X group. Here uses high-confidence Pr instead of Pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the surprise effect for each group, as well as combined across groups\n",
    "\n",
    "for group in (['immediate', 'delay']):\n",
    "    fig, ax = plt.subplots(figsize=(fig_w,fig_h), dpi=plot_dpi)\n",
    "    ax.axhline(0, color='k')\n",
    "    sns.violinplot(x='q_type', y='HC_Pr_diff',data=full_diff_df[(full_diff_df['group']==group[0].upper())&\\\n",
    "                        (full_diff_df['exc_subj']==False)], linewidth=2.5, ax=ax, \n",
    "                           order=['preS', 'preT', 'T', 'postT'], palette='Set2', saturation=1)\n",
    "    fig.suptitle('Surprise effect - ' + group + ' group', fontsize=title_font)\n",
    "    plt.xlabel('Question Type', fontsize=label_font)\n",
    "    plt.ylabel('Surprise effect (Pr-S - Pr-N)', fontsize=label_font)\n",
    "    plt.yticks(np.arange(-1,1,0.5), fontsize=tick_font)\n",
    "    plt.xticks(fontsize=tick_font)\n",
    "    if (save_figs):\n",
    "        fig.savefig(f'{fig_dir}surprise_effect_HC_{group}.eps')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(fig_w,fig_h), dpi=plot_dpi)\n",
    "ax.axhline(0, color='k')\n",
    "sns.violinplot(x='q_type', y='HC_Pr_diff',data=full_diff_df[(full_diff_df['exc_subj']==False)],\n",
    "               linewidth=2.5, ax=ax, order=['preS', 'preT', 'T', 'postT'], palette='Set2', saturation=1)\n",
    "fig.suptitle('Surprise effect - both groups', fontsize=title_font)\n",
    "plt.xlabel('Question Type', fontsize=label_font)\n",
    "plt.ylabel('Surprise effect (Pr-S - Pr-N)', fontsize=label_font)\n",
    "plt.yticks(np.arange(-1,1,0.5), fontsize=tick_font)\n",
    "plt.xticks(fontsize=tick_font)\n",
    "if (save_figs):\n",
    "    fig.savefig(f'{fig_dir}surprise_effect_HC_collapsed.eps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dataframe wth mean, sd and se of each condition and displays\n",
    "HC_mean_effect_df = full_diff_df[(full_diff_df['exc_subj']==False)].groupby(['q_type', 'group'])\\\n",
    "                        ['HC_Pr_diff'].agg(['mean', 'std', 'sem'])\n",
    "display(HC_mean_effect_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-hoc control analysis (counterbalancing)\n",
    "\n",
    "As the study was run online, multiple participants did the experiment in parallel. This resulted in the counterbalancing not being accurate, as some participants stopped in the middle of the experiment. Since this could lead to spurious results, we include an additional test to assess whether the proactive effect is driven by the inaccurate counterbalancing - a single trial analysis using brms, accounting for order and version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displays counterbalancing info\n",
    "\n",
    "To assess the accuracy of counterbalancing, displays the number of participants that viewed the films in each of the possible orders (six) and each of the possible versions (four versions, with different divisions of target scenes to neutral/surprise). Also displays the range of the number of participants in the 24 combinations of order and version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of participants in each combination of order X version X group\n",
    "cb_info_full = full_subj_avg_df[~full_subj_avg_df['exc_subj']][['subj','order_idx','mov_ver','group']].drop_duplicates().groupby(\\\n",
    "                    ['order_idx','mov_ver','group'])['subj'].nunique().reset_index()\\\n",
    "                    .rename(columns={'subj':'n_subj'})\n",
    "\n",
    "# Counts the number of participants that viewed the films in each order and in each version, per group\n",
    "order_counts_I = np.array(cb_info_full[cb_info_full['group']=='I'].groupby(['order_idx'])['n_subj'].sum())\n",
    "ver_counts_I = np.array(cb_info_full[cb_info_full['group']=='I'].groupby(['mov_ver'])['n_subj'].sum())\n",
    "order_counts_D = np.array(cb_info_full[cb_info_full['group']=='D'].groupby(['order_idx'])['n_subj'].sum())\n",
    "ver_counts_D = np.array(cb_info_full[cb_info_full['group']=='D'].groupby(['mov_ver'])['n_subj'].sum())\n",
    "\n",
    "# Displays the results\n",
    "print('Range of participants in orderXverXgroup combinations: {}-{}\\n'.format(min(cb_info_full['n_subj']),\\\n",
    "                                                                         max(cb_info_full['n_subj'])))\n",
    "print('Immediate group - number of participants per order/version:')\n",
    "print ('  Number of participants who viewed films in orders 1-6: ' + np.array2string(order_counts_I))\n",
    "print ('  Number of participants who viewed films in versions 1-4: ' + np.array2string(ver_counts_I))\n",
    "print('\\nDelay group - number of participants per order/version:')\n",
    "print ('  Number of participants who viewed films in orders 1-6: ' + np.array2string(order_counts_D))\n",
    "print ('  Number of participants who viewed films in versions 1-4: ' + np.array2string(ver_counts_D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single trial analysis - data preparation\n",
    "\n",
    "Prepares the dataframes required for the single trial analyses and saves in a csv that can later be read by the R brms script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenates the immediate and delay groups of all batches, creating one full dataframe\n",
    "test_I = pd.concat([res['test_df'] for res in immediate_res])\n",
    "test_D = pd.concat([res['test_df'] for res in delay_res])\n",
    "\n",
    "# The subject numbers are restarted in each batch for flexibility (e.g. concatenating \n",
    "# only dataframes of one group), so updates the subject number to run across all batches\n",
    "# of both groups, then concatenates both groups into a single dataframe\n",
    "test_I['subj'] = test_I['subj'] + (test_I['batch']-1)*batch_n_subjects*2\n",
    "test_D['subj'] = test_D['subj'] + (test_D['batch']-1)*batch_n_subjects*2 + batch_n_subjects\n",
    "full_test_df = pd.concat((test_I, test_D)).reset_index(drop=True)\n",
    "    \n",
    "\n",
    "# Merges information from the test_df with the surprise_dist_df \n",
    "# to aggregate the information for the lm analysis in one dataframe\n",
    "brms_df = full_test_df.astype({'non_foil_q':'int', 'mov_ver':'int'}).merge(surprise_dist_df[['non_foil_q',\\\n",
    "                'mov_ver', 'prev_S_dist', 'next_S_dist']].drop_duplicates(), on=['non_foil_q', 'mov_ver'], \n",
    "                validate='many_to_one')\n",
    "\n",
    "# Adds a column of whether the participant answered 'old' for the brms analysis. For HC analysis the column\n",
    "# indicates whether the participant answered old with high confidence\n",
    "brms_df['ansOld'] = brms_df['conf_level']>0\n",
    "brms_df['HC_ansOld'] = brms_df['conf_level']==2\n",
    "\n",
    "# Takes only the subset of relevant columns and sets the dypes of some of them, otherwise this causes\n",
    "# incorrect conversion to an R dataframe. This code is commented out since the notebook crashes when\n",
    "# trying to run the R code in it. Left here commented so other tests can be run in the noteboko if needed\n",
    "# brms_df.drop(columns=['target_q', 'batch', 'conf_level', 'mov_ver'], inplace=True)\n",
    "# brms_df = brms_df.astype({'subj':'int', 'exc_subj':'bool', 'non_foil_q':'int', 'action_idx':'int'})\n",
    "\n",
    "# Saves as a csv file\n",
    "brms_df.to_csv(brms_dir + 'brms_df.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single trial analysis - model preparation\n",
    "Creates initial models (the first chain) as well as submission scripts so the full models can be parallelised. Then submits the jobs to the cluster.\n",
    "As the notebook crashes when running the code, runs an R script called create_brms_models.R that should be placed in the R_dir. Once it has finished, submits the jobs.\n",
    "\n",
    "The R script is a slight modification of code written by Alex Quent:\n",
    "https://github.com/JAQuent/bayesianMLM/tree/master/CBU_clusterGuide\n",
    "\n",
    "___Important:___ \n",
    "1. This requires a slurm parallelisation system. If anyone would like to run this and doesn't have slurm, please contact me and I will try to help\n",
    "2. This part will take a long time, without printing anything to the screen\n",
    "3. When the cell stops running, it doesn't indicate the R script has finished. The following cell can be used to verify all jobs have finished before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the R script\n",
    "R_exit_code = os.system('Rscript {}{} {}'.format(R_dir, create_models_fname, brms_dir))\n",
    "\n",
    "# Submits the jobs created by the script, or displays an error if the R code exited with a non-zero exit code\n",
    "if (R_exit_code != 0):\n",
    "    print(f'Rscript exited with non-zero code {R_exit_code}, try running directly from shell to view error')\n",
    "else:\n",
    "    submit_scripts = glob.glob(brms_dir + '_rslurm*/submit.sh')\n",
    "    if (len(submit_scripts)==0):\n",
    "        print('Rscript did not execute properly, try running directly from shell to view error')\n",
    "    else:\n",
    "        for submit_fname in submit_scripts:\n",
    "            os.system('sbatch --cpus-per-task 8 ' + submit_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single trial analysis - test whether jobs have finished\n",
    "Checks whether the resulting files of the jobs have been created. The cell is set to be run manually, otherwise would require an infinite loop that keeps checking when the files are created and could cause the notebook to crash. Note, if the analysis is rerun for some reason - the files should be deleted before rerunning (otherwise the test below won't work). Once this cell prints that all jobs have finished, it will be possible to proceed to the hypothesis testing that follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks first if the scripts have been created\n",
    "if (len(submit_scripts)==0):\n",
    "    print ('No scripts have been created yet')\n",
    "\n",
    "else:\n",
    "    # For each of the submit scripts created - checks whether a file called results_0.RDS has been created\n",
    "    all_created = True\n",
    "    for submit_fname in submit_scripts:\n",
    "        if (not os.path.exists(os.path.dirname(submit_fname) + '/results_0.RDS')):\n",
    "            print ('{}/results_0.RDS not yet created'.format(os.path.dirname(submit_fname)))\n",
    "            all_created = False\n",
    "\n",
    "    if (all_created):\n",
    "        print('All jobs have finished - proceed to next step')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single trial analysis - test hypotheses\n",
    "Combines the different chains of each model and tests the relevant hypotheses.\n",
    "\n",
    "This code is a slight modification of code written by Alex Quent:\n",
    "https://github.com/JAQuent/bayesianMLM/tree/master/CBU_clusterGuide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i brms_dir\n",
    "\n",
    "# Sets seed to arbitrary value for reproducibility\n",
    "set.seed(26)\n",
    "\n",
    "# Results folder prefix\n",
    "pattern <- '_rslurm_'\n",
    "\n",
    "\n",
    "# Gets all rslurm folders in the brms directory\n",
    "allFiles      <- list.files(brms_dir)\n",
    "rslurmFolders <- allFiles[grepl(pattern, allFiles)]\n",
    "modelNames    <- gsub(paste0(pattern, '(.+)'), '\\\\1', rslurmFolders)\n",
    "\n",
    "# Goes through all rslurm folders\n",
    "for(j in 1:length(rslurmFolders)){\n",
    "    \n",
    "    # Reads the rlsurm results file\n",
    "    tempList <- readRDS(paste0(brms_dir, '/', rslurmFolders[j], '/results_0.RDS'), refhook = NULL)\n",
    "\n",
    "    # Combines all runs of the model\n",
    "    tempModel_args <- list()\n",
    "    for (k in 1:length(tempList)) {\n",
    "      tempModel_args[[k]] <- tempList[[k]]$model\n",
    "    }\n",
    "    tempModel <- combine_models(mlist=tempModel_args)\n",
    "\n",
    "    # Assign model name based on folder name\n",
    "    assign(modelNames[j], tempModel)\n",
    "}\n",
    "\n",
    "# Clears variables\n",
    "rm('tempList')\n",
    "rm('tempModel')\n",
    "rm('tempModel_args')\n",
    "\n",
    "# Saves the combined model to the brms directory\n",
    "save.image(paste(brms_dir, 'combinedModels.RData', sep=\"/\"))\n",
    "\n",
    "# Runs three tests equivalent to the analysis on the averaged proactive data, once for regular\n",
    "# confidence and once for high confidence\n",
    "# - main effect of surprise\n",
    "# - surpriseXq-type interaction\n",
    "# - surpriseXq-typeXgroup interaction\n",
    "pro_S_bf = bayes_factor(model_pro_w_S, model_pro_no_S)\n",
    "pro_SQT_bf = bayes_factor(model_pro_no_3way, model_pro_no_SQT)\n",
    "pro_3way_bf = bayes_factor(model_pro_full, model_pro_no_3way)\n",
    "HC_pro_S_bf = bayes_factor(model_HC_pro_w_S, model_HC_pro_no_S)\n",
    "HC_pro_SQT_bf = bayes_factor(model_HC_pro_no_3way, model_HC_pro_no_SQT)\n",
    "HC_pro_3way_bf = bayes_factor(model_HC_pro_full, model_HC_pro_no_3way)\n",
    "\n",
    "# Prints the results\n",
    "print(paste0('Evidence for/against main effect of surprise: ', format(pro_S_bf$bf, digits=2), \n",
    "             '/', format(1/pro_S_bf$bf, digits=2)))\n",
    "print(paste0('Evidence for/against surprise/Q-type interaction: ', format(pro_SQT_bf$bf, digits=2), \n",
    "            '/', format(1/pro_SQT_bf$bf, digits=2)))\n",
    "print(paste0('Evidence for/against 3-way interaction: ', format(pro_3way_bf$bf, digits=2), \n",
    "             '/', format(1/pro_3way_bf$bf, digits=2)))\n",
    "print(paste0('Evidence for/against main effect of surprise (HC): ', format(HC_pro_S_bf$bf, digits=2), \n",
    "             '/', format(1/HC_pro_S_bf$bf, digits=2)))\n",
    "print(paste0('Evidence for/against surprise/Q-type interaction (HC): ', format(HC_pro_SQT_bf$bf, digits=2), \n",
    "            '/', format(1/HC_pro_SQT_bf$bf, digits=2)))\n",
    "print(paste0('Evidence for/against 3-way interaction (HC): ', format(HC_pro_3way_bf$bf, digits=2), \n",
    "             '/', format(1/HC_pro_3way_bf$bf, digits=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional statistical tests\n",
    "Runs several calculations of interest that provide additional information, for a more complete picture, but these are not part of the predefined stats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional stats (R)\n",
    "Runs the following calculations (both for Pr and for HC-Pr):\n",
    "- Tests for a groupXsurprise interaction in the retroactive 3-way anova\n",
    "- Tests for effects of surprise separated by group (even if there is no current evidence in favour of interactions involving group). These include both a main effect of surprise and an interaction of question-type and surprise within each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i full_subj_avg_df -i full_diff_df\n",
    "\n",
    "# Sets seed to arbitrary value at the beginning of each R cell, otherwise some results won't be reproducible\n",
    "set.seed(26)\n",
    "\n",
    "# Turns the subj column into a factor - will be converted to numeric by default\n",
    "# Also sets the others to be factors (though they're already strings), since something\n",
    "# in the conversion in the notebook works differently and at least one isn't recognised \n",
    "# as a factor otherwise\n",
    "full_subj_avg_df[,'subj'] <- as.factor(full_subj_avg_df[,'subj'])\n",
    "full_subj_avg_df[,'q_type'] <- as.factor(full_subj_avg_df[,'q_type'])\n",
    "full_subj_avg_df[,'surprise_type'] <- as.factor(full_subj_avg_df[,'surprise_type'])\n",
    "full_subj_avg_df[,'group'] <- as.factor(full_subj_avg_df[,'group'])\n",
    "\n",
    "# Separates the retroactive data by group (removing excluded subjects)\n",
    "retroactive_data_I <- subset(full_subj_avg_df, (q_type=='preS' | q_type=='preT') & \n",
    "                             exc_subj=='FALSE' & group=='I')\n",
    "retroactive_data_D <- subset(full_subj_avg_df, (q_type=='preS' | q_type=='preT') & \n",
    "                             exc_subj=='FALSE' & group=='D')\n",
    "\n",
    "# Runs the following anovas (assumes the full retroactive one has already been run):\n",
    "# - A 2-way anova with question-type and surprise for the retroactive data of the immediate group\n",
    "# - A 2-way anova with question-type and surprise for the retroactive data of the delay group\n",
    "retro_I_bf = anovaBF(Pr ~ q_type*surprise_type + subj, \n",
    "                     data=retroactive_data_I, whichRandom='subj')\n",
    "retro_D_bf = anovaBF(Pr ~ q_type*surprise_type + subj, \n",
    "                     data=retroactive_data_D, whichRandom='subj')\n",
    "retro_HC_I_bf = anovaBF(HC_Pr ~ q_type*surprise_type + subj, \n",
    "                        data=retroactive_data_I, whichRandom='subj')\n",
    "retro_HC_D_bf = anovaBF(HC_Pr ~ q_type*surprise_type + subj, \n",
    "                        data=retroactive_data_D, whichRandom='subj')\n",
    "\n",
    "# Extracts BF in favour of the alternative (H1) for each of the tests described above\n",
    "bf_retro_group_surprise_H1 = as.numeric(as.vector(retro_bf[17]/retro_bf[15]))\n",
    "bf_retro_I_main_surprise_H1 = as.numeric(as.vector(retro_I_bf[3]/retro_I_bf[1]))\n",
    "bf_retro_D_main_surprise_H1 = as.numeric(as.vector(retro_D_bf[3]/retro_D_bf[1]))\n",
    "bf_retro_I_surprise_q_type_H1 = as.numeric(as.vector(retro_I_bf[4]/retro_I_bf[3]))\n",
    "bf_retro_D_surprise_q_type_H1 = as.numeric(as.vector(retro_D_bf[4]/retro_D_bf[3]))\n",
    "bf_HC_retro_group_surprise_H1 = as.numeric(as.vector(retro_HC_bf[17]/retro_HC_bf[15]))\n",
    "bf_HC_retro_I_main_surprise_H1 = as.numeric(as.vector(retro_HC_I_bf[3]/retro_HC_I_bf[1]))\n",
    "bf_HC_retro_D_main_surprise_H1 = as.numeric(as.vector(retro_HC_D_bf[3]/retro_HC_D_bf[1]))\n",
    "bf_HC_retro_I_surprise_q_type_H1 = as.numeric(as.vector(retro_HC_I_bf[4]/retro_HC_I_bf[3]))\n",
    "bf_HC_retro_D_surprise_q_type_H1 = as.numeric(as.vector(retro_HC_D_bf[4]/retro_HC_D_bf[3]))\n",
    "\n",
    "# Makes sure the correct models were taken from the anovas (in case the order can change)\n",
    "add_validated <- TRUE\n",
    "add_validated <- add_validated & setequal(unlist(strsplit(names(as.vector(retro_bf[15])), ' + ', TRUE)),\n",
    "                        c('group','q_type','surprise_type', 'group:q_type', 'q_type:surprise_type','subj'))\n",
    "add_validated <- add_validated & setequal(unlist(strsplit(names(as.vector(retro_I_bf[4])), ' + ', TRUE)), \n",
    "                        c('q_type','surprise_type', 'q_type:surprise_type','subj'))\n",
    "add_validated <- add_validated & setequal(unlist(strsplit(names(as.vector(retro_I_bf[3])), ' + ', TRUE)), \n",
    "                        c('q_type','surprise_type','subj'))\n",
    "add_validated <- add_validated & setequal(unlist(strsplit(names(as.vector(retro_I_bf[1])), ' + ', TRUE)), \n",
    "                        c('q_type','subj'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional stats - display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls the relevant variables from the R code and converts to python native data types\n",
    "\n",
    "%Rpull add_validated bf_retro_group_surprise_H1 bf_retro_I_main_surprise_H1 bf_retro_D_main_surprise_H1 \\\n",
    "bf_retro_I_surprise_q_type_H1 bf_retro_D_surprise_q_type_H1 bf_HC_retro_group_surprise_H1 \\\n",
    "bf_HC_retro_I_main_surprise_H1 bf_HC_retro_D_main_surprise_H1 bf_HC_retro_I_surprise_q_type_H1 \\\n",
    "bf_HC_retro_D_surprise_q_type_H1\n",
    "\n",
    "add_validated = bool(np.asarray(add_validated)[0])\n",
    "bf_retro_group_surprise_H1 = np.asarray(bf_retro_group_surprise_H1)[0]\n",
    "bf_retro_I_main_surprise_H1 = np.asarray(bf_retro_I_main_surprise_H1)[0]\n",
    "bf_retro_D_main_surprise_H1 = np.asarray(bf_retro_D_main_surprise_H1)[0]\n",
    "bf_retro_I_surprise_q_type_H1 = np.asarray(bf_retro_I_surprise_q_type_H1)[0]\n",
    "bf_retro_D_surprise_q_type_H1 = np.asarray(bf_retro_D_surprise_q_type_H1)[0]\n",
    "bf_HC_retro_group_surprise_H1 = np.asarray(bf_HC_retro_group_surprise_H1)[0]\n",
    "bf_HC_retro_I_main_surprise_H1 = np.asarray(bf_HC_retro_I_main_surprise_H1)[0]\n",
    "bf_HC_retro_D_main_surprise_H1 = np.asarray(bf_HC_retro_D_main_surprise_H1)[0]\n",
    "bf_HC_retro_I_surprise_q_type_H1 = np.asarray(bf_HC_retro_I_surprise_q_type_H1)[0]\n",
    "bf_HC_retro_D_surprise_q_type_H1 = np.asarray(bf_HC_retro_D_surprise_q_type_H1)[0]\n",
    "\n",
    "\n",
    "# Prints results of calculations, first verifying the chosen models are the correct ones\n",
    "if (not add_validated):\n",
    "    print('The ANOVA models chosen for the calculations do not match the expected ones.' + \\\n",
    "         'This may be due to different platforms/versions of the BayesFactor package.' + \\\n",
    "          'View cell above to identify the correct models from the bf variable (that includes)' + \\\n",
    "          'all models in the anova')\n",
    "else:\n",
    "    print('Results of additional tests, all retroactive (H1/H0):')\n",
    "    print('   Evidence of a groupXsurprise interaction: '\\\n",
    "              + '(%.3g/%.3g)'% (bf_retro_group_surprise_H1, 1/bf_retro_group_surprise_H1))\n",
    "    print('   Evidence of a main effect of surprise (immediate group): '\\\n",
    "              + '(%.3g/%.3g)'% (bf_retro_I_main_surprise_H1, 1/bf_retro_I_main_surprise_H1))\n",
    "    print('   Evidence of a main effect of surprise (delay group): '\\\n",
    "              + '(%.3g/%.3g)'% (bf_retro_D_main_surprise_H1, 1/bf_retro_D_main_surprise_H1))\n",
    "    print('   Evidence of a q-typeXsurprise interaction (immediate group): '\\\n",
    "              + '(%.3g/%.3g)'% (bf_retro_I_surprise_q_type_H1, 1/bf_retro_I_surprise_q_type_H1))\n",
    "    print('   Evidence of a q-typeXsurprise interaction (delay group): '\\\n",
    "              + '(%.3g/%.3g)'% (bf_retro_D_surprise_q_type_H1, 1/bf_retro_D_surprise_q_type_H1))\n",
    "    print('\\nResults of additional tests, all retroactive - HC Pr (H1/H0):')\n",
    "    print('   Evidence of a groupXsurprise interaction: '\\\n",
    "              + '(%.3g/%.3g)'% (bf_HC_retro_group_surprise_H1, 1/bf_HC_retro_group_surprise_H1))\n",
    "    print('   Evidence of a main effect of surprise (immediate group): '\\\n",
    "              + '(%.3g/%.3g)'% (bf_HC_retro_I_main_surprise_H1, 1/bf_HC_retro_I_main_surprise_H1))\n",
    "    print('   Evidence of a main effect of surprise (delay group): '\\\n",
    "              + '(%.3g/%.3g)'% (bf_HC_retro_D_main_surprise_H1, 1/bf_HC_retro_D_main_surprise_H1))\n",
    "    print('   Evidence of a q-typeXsurprise interaction (immediate group): '\\\n",
    "              + '(%.3g/%.3g)'% (bf_HC_retro_I_surprise_q_type_H1, 1/bf_HC_retro_I_surprise_q_type_H1))\n",
    "    print('   Evidence of a q-typeXsurprise interaction (delay group): '\\\n",
    "              + '(%.3g/%.3g)'% (bf_HC_retro_D_surprise_q_type_H1, 1/bf_HC_retro_D_surprise_q_type_H1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity checks\n",
    "Runs various tests to validate the results, including testing that the dataframes have the expected number of entries and that the Bayesian stats are comparable to frequentists stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Makes sure dataframes include the expected number of entries\n",
    "Most sanity checks are within the process_batch function, but since aggregation across batches is done in the notebook, adds a few tests here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of the dataframes (full_subj_avg_df full_diff_df first_batch_df target_diff_df), makes sure\n",
    "# the following are as expected (in this order):\n",
    "#  - list of unique subjects\n",
    "#  - number of entries per subject\n",
    "\n",
    "df_verifiied = True\n",
    "\n",
    "n_q_type = 4\n",
    "n_surprise_type = 2\n",
    "total_n_subj = batch_num*2*batch_n_subjects\n",
    "first_batch_n_subj = 2*batch_n_subjects\n",
    "\n",
    "# Counts the number of question pairs (foil + non-foil counted once)\n",
    "n_q_pairs = len(np.unique(immediate_res[0]['q_info_df']['non_foil_q']))\n",
    "\n",
    "# full_subj_avg_df verification\n",
    "df_verifiied = df_verifiied and np.array_equal(np.unique(full_subj_avg_df['subj']),np.arange(total_n_subj))\n",
    "df_verifiied = df_verifiied and (full_subj_avg_df.groupby(['subj', 'q_type', 'surprise_type']).ngroups \\\n",
    "                                == total_n_subj*n_q_type*n_surprise_type)\n",
    "\n",
    "# full_diff_df verification\n",
    "df_verifiied = df_verifiied and np.array_equal(np.unique(full_diff_df['subj']),np.arange(total_n_subj))\n",
    "df_verifiied = df_verifiied and (full_diff_df.groupby(['subj', 'q_type']).ngroups \\\n",
    "                                == total_n_subj*n_q_type)\n",
    "\n",
    "# first_batch_df verification\n",
    "df_verifiied = df_verifiied and np.array_equal(np.unique(first_batch_df['subj']),np.arange(first_batch_n_subj))\n",
    "df_verifiied = df_verifiied and (first_batch_df.groupby(['subj', 'q_type', 'surprise_type']).ngroups \\\n",
    "                                == first_batch_n_subj*n_q_type*n_surprise_type)\n",
    "\n",
    "# target_diff_df verification\n",
    "df_verifiied = df_verifiied and np.array_equal(np.unique(target_diff_df['subj']),np.arange(first_batch_n_subj))\n",
    "df_verifiied = df_verifiied and (target_diff_df.groupby(['subj', 'q_type']).ngroups \\\n",
    "                                == first_batch_n_subj)\n",
    "\n",
    "\n",
    "# brms_df verification\n",
    "df_verifiied = df_verifiied and np.array_equal(np.unique(brms_df['subj']),np.arange(total_n_subj))\n",
    "df_verifiied = df_verifiied and (brms_df.groupby(['subj', 'non_foil_q']).ngroups \\\n",
    "                                == total_n_subj*n_q_pairs)\n",
    "\n",
    "if (not df_verifiied):\n",
    "    print ('Problem in dataframe verification')\n",
    "else:\n",
    "    print ('Sanity checks ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifies surprise>neutral memory\n",
    "\n",
    "Verifies that there is better memory for surprising targets relative to neutral ones. This was verified for the first batch, here makes sure the effect remains for the entire group of participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i full_diff_df\n",
    "\n",
    "# Sets seed to arbitrary value at the beginning of each R cell, otherwise some results won't be reproducible\n",
    "set.seed(26)\n",
    "\n",
    "# Turns the subj column into a factor - will be converted to numeric by default\n",
    "# Also sets the others to be factors (though they're already strings), since something\n",
    "# in the conversion in the notebook works differently and at least one isn't recognised \n",
    "# as a factor otherwise\n",
    "full_diff_df[,'subj'] <- as.factor(full_diff_df[,'subj'])\n",
    "full_diff_df[,'q_type'] <- as.factor(full_diff_df[,'q_type'])\n",
    "full_diff_df[,'group'] <- as.factor(full_diff_df[,'group'])\n",
    "\n",
    "# Slices the data to have the relevant subsets for each test\n",
    "\n",
    "T_I_diff <- subset(full_diff_df, q_type=='T' & group=='I' & exc_subj=='FALSE')\n",
    "T_D_diff <- subset(full_diff_df, q_type=='T' & group=='D' & exc_subj=='FALSE')\n",
    "T_comb_diff <- subset(full_diff_df, q_type=='T' & exc_subj=='FALSE')\n",
    "\n",
    "\n",
    "# Runs the planned t-tests, looking at surprise effects within each question type\n",
    "\n",
    "T_I_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(T_I_diff['Pr_diff'])))))\n",
    "T_D_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(T_D_diff['Pr_diff'])))))\n",
    "T_comb_bf_H1 <- as.numeric(as.vector(ttestBF(as.numeric(unlist(T_comb_diff['Pr_diff'])))))\n",
    "\n",
    "cat('Evidence in favour of better memory for surprising targets:\\n')\n",
    "cat(sprintf('  Immediate group: %.2e\\n', T_I_bf_H1))\n",
    "cat(sprintf('  Delay group: %.2e\\n', T_D_bf_H1))\n",
    "cat(sprintf('  Combined %.2e\\n', T_comb_bf_H1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis - dependency\n",
    "\n",
    "Runs an exploratory analysis (not included in Stage 1 of the registered report) to test whether surprising actions segment the episode, serving as boundaries. To do so, tests the dependency between the different question types - checking whether the dependency changes for surprising events vs. neutral ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis - data preparation\n",
    "\n",
    "Prepares the data for the dependency analysis - creates a dataframe in which the hits of all questions from the same scene are in one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivots table to create a separate column for each question type, then renames the columns \n",
    "# (merging into one level of columns) and resets index. Takes only non-foils for this analysis\n",
    "dep_df = brms_df[~(brms_df['foil'].astype(bool))].rename(columns={'ansOld':'hit', 'HC_ansOld': 'HC_hit'})\\\n",
    "            .pivot_table(index=['subj', 'group', 'exc_subj', 'target_q', 'surprise_type'], \n",
    "                                           columns='q_type', values=['hit', 'HC_hit'])\n",
    "\n",
    "dep_df.columns = list(map(\"_\".join, dep_df.columns))\n",
    "dep_df.reset_index(inplace=True)\n",
    "\n",
    "# Divides the dependency df by surprise - creating two dataframes\n",
    "dep_df_S = dep_df[(dep_df['surprise_type']=='S')].drop(columns='surprise_type')\n",
    "dep_df_N = dep_df[(dep_df['surprise_type']=='N')].drop(columns='surprise_type')\n",
    "\n",
    "# Calculates the average of each column and adds back to the dependency dfs so these can be added\n",
    "# as predictors, separating the average subject memory from the per-question effects which\n",
    "# demonstrate whether there's dependency\n",
    "dep_df_S_avg = dep_df_S.groupby(['subj', 'group', 'exc_subj']).mean().drop(columns='target_q')\n",
    "dep_df_S_avg.columns = ['avg_' + c for c in dep_df_S_avg.columns]\n",
    "dep_df_N_avg = dep_df_N.groupby(['subj', 'group', 'exc_subj']).mean().drop(columns='target_q')\n",
    "dep_df_N_avg.columns = ['avg_' + c for c in dep_df_N_avg.columns]\n",
    "dep_df_S = dep_df_S.merge(dep_df_S_avg.reset_index(), on=['subj', 'group', 'exc_subj'])\n",
    "dep_df_N = dep_df_N.merge(dep_df_N_avg.reset_index(), on=['subj', 'group', 'exc_subj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displayes the distribution of average hit rates for the surprising targets to estimate whether it is\n",
    "# meaningful to include targets in the dependency analysis.\n",
    "\n",
    "all_hit_rate = full_subj_avg_df[(full_subj_avg_df['q_type']=='T')&(full_subj_avg_df['surprise_type']=='S')&\\\n",
    "                          (~full_subj_avg_df['exc_subj'])]['hit_rate']\n",
    "hit_rates, hr_n_subj = np.unique(all_hit_rate, return_counts=True)\n",
    "hr_df = pd.DataFrame.from_dict({'hit_rate':hit_rates, 'n_subj':hr_n_subj})\n",
    "hr_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis - dependency calculation (R)\n",
    "\n",
    "Calculates the dependency using general linear mixed models - to calculate the dependency between hits in one question type (qt1) and another (qt2), calculates the coefficients of qt2 in a model that has qt1 as a DV and the other question types as predictors (all binary - hit/miss per question per participant). Compares this to models in which the pairing of qt2 to qt1 has been randomly shuffled per participant. \n",
    "\n",
    "Runs this separately for neutral and surprising scenes to assess whether surprise reduces the dependency. The main question of interest is whether a surprising event affects dependency between preT and postT questions, that straddle the surprise, and as a control tests preS and preT (for completeness calculates dependency for all pairs of question types, excluding targets)\n",
    "\n",
    "___Important:___ \n",
    "1. This requires a slurm parallelisation system. If anyone would like to run this and doesn't have slurm, please contact me and I will try to help\n",
    "2. This part will take a long time, without printing anything to the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i dep_df_S -i dep_df_N -i n_nodes -i cpus_per_node -i n_iter\n",
    "\n",
    "# Sets seed for reproducibility\n",
    "set.seed(26)\n",
    "\n",
    "# Turns columns into factors\n",
    "dep_df_S[,'subj'] <- as.factor(dep_df_S[,'subj'])\n",
    "dep_df_S[,'group'] <- as.factor(dep_df_S[,'group'])\n",
    "dep_df_S[,'target_q'] <- as.factor(dep_df_S[,'target_q'])\n",
    "dep_df_N[,'subj'] <- as.factor(dep_df_N[,'subj'])\n",
    "dep_df_N[,'group'] <- as.factor(dep_df_N[,'group'])\n",
    "dep_df_N[,'target_q'] <- as.factor(dep_df_N[,'target_q'])\n",
    "\n",
    "# Gets separate subsets of the surprising/neutral data\n",
    "dep_df_S <- subset(dep_df_S, exc_subj=='FALSE')\n",
    "dep_df_N <- subset(dep_df_N, exc_subj=='FALSE')\n",
    "\n",
    "# Defines the names of the predictors for each question type\n",
    "q_types <- c('hit_preS', 'hit_preT', 'hit_postT')\n",
    "HC_q_types <- c('HC_hit_preS', 'HC_hit_preT', 'HC_hit_postT')\n",
    "n_q_types <- length(q_types)\n",
    "\n",
    "# Variables to hold coefficients of the intact models such that array in locations \n",
    "# [qt1, qt2] will hold the coefficient of qt2 when modeling qt1\n",
    "intact_coef_S <- array(NaN, dim=c(n_q_types,n_q_types))\n",
    "intact_coef_N <- array(NaN, dim=c(n_q_types,n_q_types))\n",
    "intact_coef_HC_S <- array(NaN, dim=c(n_q_types,n_q_types))\n",
    "intact_coef_HC_N <- array(NaN, dim=c(n_q_types,n_q_types))\n",
    "\n",
    "for (qt1 in 1:n_q_types) {\n",
    "\n",
    "    # Calculates the intact models\n",
    "    qt1_S_model <- glmer(as.formula(paste(q_types[qt1], ' ~ ', paste(q_types[setdiff(seq(1,n_q_types), \n",
    "                qt1)], collapse=' + '), ' + group + (1|subj) + (1|target_q)')), \n",
    "                data=dep_df_S, family=binomial, control=glmerControl(optimizer='bobyqa', \n",
    "                optCtrl=list(maxfun=2e5)))\n",
    "    qt1_N_model <- glmer(as.formula(paste(q_types[qt1], ' ~ ', paste(q_types[setdiff(seq(1,n_q_types), \n",
    "                qt1)], collapse=' + '), ' + group + (1|subj) + (1|target_q)')), \n",
    "                data=dep_df_N, family=binomial, control=glmerControl(optimizer='bobyqa', \n",
    "                optCtrl=list(maxfun=2e5)))\n",
    "\n",
    "    # Same for HC\n",
    "    HC_qt1_S_model <- glmer(as.formula(paste(HC_q_types[qt1], ' ~ ', paste(HC_q_types[setdiff(seq(1,n_q_types), \n",
    "                qt1)], collapse=' + '), ' + group + (1|subj) + (1|target_q)')), \n",
    "                data=dep_df_S, family=binomial, control=glmerControl(optimizer='bobyqa', \n",
    "                optCtrl=list(maxfun=2e5)))\n",
    "    HC_qt1_N_model <- glmer(as.formula(paste(HC_q_types[qt1], ' ~ ', paste(HC_q_types[setdiff(seq(1,n_q_types), \n",
    "                qt1)], collapse=' + '), ' + group + (1|subj) + (1|target_q)')), \n",
    "                data=dep_df_N, family=binomial, control=glmerControl(optimizer='bobyqa', \n",
    "                optCtrl=list(maxfun=2e5)))\n",
    "\n",
    "    # Iterates over the other predictors and extracts the coefficients\n",
    "    for (qt2 in setdiff(1:length(q_types), qt1)) {\n",
    "        intact_coef_S[qt1,qt2] <- as.numeric(fixef(qt1_S_model)[grep(q_types[qt2], names(fixef(qt1_S_model)))])\n",
    "        intact_coef_N[qt1,qt2] <- as.numeric(fixef(qt1_N_model)[grep(q_types[qt2], names(fixef(qt1_N_model)))])\n",
    "        intact_coef_HC_S[qt1,qt2] <- as.numeric(fixef(HC_qt1_S_model)[grep(HC_q_types[qt2], names(fixef(HC_qt1_S_model)))])\n",
    "        intact_coef_HC_N[qt1,qt2] <- as.numeric(fixef(HC_qt1_N_model)[grep(HC_q_types[qt2], names(fixef(HC_qt1_N_model)))])\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Creates two variables with the parameters for the parallelisation - first pairs of \n",
    "# question-types (qt1 will be the DV and qt2 will be the shuffled predictor), then random\n",
    "# seeds. Uses the same seed for each of the 6 pairs (though this isn't necessary)\n",
    "qt_pairs <- combn(seq(n_q_types),2)\n",
    "qt_pairs <- cbind(qt_pairs, qt_pairs[c(2,1),])\n",
    "n_pairs <- ncol(qt_pairs)\n",
    "iter_idx <- as.vector(t(replicate(n_pairs,seq(n_iter))))\n",
    "qt_pairs <- qt_pairs[,rep(seq(n_pairs), n_iter)]\n",
    "rnd_seeds <- sample(99999, n_iter)\n",
    "rnd_seeds <- as.vector(t(replicate(n_pairs,rnd_seeds)))\n",
    "\n",
    "calc_glmer <- function(i, qt1, qt2, seed) {\n",
    "  \n",
    "  # Sets for reproducibility\n",
    "  set.seed(seed)\n",
    "  \n",
    "  # Creates versions of the dataframes in which qt2 columns are shuffled within subject\n",
    "  rnd_S <- ddply(dep_df_S[,c('subj', q_types[qt2], HC_q_types[qt2])], .(subj), \n",
    "                 function(x) x[sample(nrow(x),nrow(x)),])\n",
    "  rnd_N <- ddply(dep_df_N[,c('subj', q_types[qt2], HC_q_types[qt2])], .(subj), \n",
    "                 function(x) x[sample(nrow(x),nrow(x)),])\n",
    "  \n",
    "  # Creates versions of the dataframes in which qt2 has been shuffled. Modifies regular and HC in the same DF,\n",
    "  # though these will be used in separate models\n",
    "  curr_qt2_rnd_S <- dep_df_S\n",
    "  curr_qt2_rnd_S[,q_types[qt2]] <- rnd_S[,q_types[qt2]]\n",
    "  curr_qt2_rnd_S[,HC_q_types[qt2]] <- rnd_S[,HC_q_types[qt2]]\n",
    "  curr_qt2_rnd_N <- dep_df_N\n",
    "  curr_qt2_rnd_N[,q_types[qt2]] <- rnd_N[,q_types[qt2]]\n",
    "  curr_qt2_rnd_N[,HC_q_types[qt2]] <- rnd_N[,HC_q_types[qt2]]\n",
    "  \n",
    "  # Runs an glmer with qt1 as the dependent variable and qt2 shuffled, once for surprising and once for neutral\n",
    "  # scenes. Then runs the same for HC\n",
    "  qt2_rnd_S_model <- glmer(as.formula(paste(q_types[qt1], ' ~ ', paste(q_types[setdiff(seq(1,length(q_types)), \n",
    "                           qt1)], collapse=' + '), ' + group + (1|subj) + (1|target_q)')), \n",
    "                           data=curr_qt2_rnd_S, family=binomial, control=glmerControl(optimizer='bobyqa',\n",
    "                           optCtrl=list(maxfun=2e5)))\n",
    "  qt2_rnd_N_model <- glmer(as.formula(paste(q_types[qt1], ' ~ ', paste(q_types[setdiff(seq(1,length(q_types)),\n",
    "                           qt1)], collapse=' + '), ' + group + (1|subj) + (1|target_q)')), \n",
    "                           data=curr_qt2_rnd_N, family=binomial, \n",
    "                           control=glmerControl(optimizer='bobyqa', optCtrl=list(maxfun=2e5)))\n",
    "  HC_qt2_rnd_S_model <- glmer(as.formula(paste(HC_q_types[qt1], ' ~ ', \n",
    "                            paste(HC_q_types[setdiff(seq(1,length(q_types)), qt1)], collapse=' + '),\n",
    "                            ' +  group + (1|subj) + (1|target_q)')), data=curr_qt2_rnd_S, \n",
    "                            family=binomial, control=glmerControl(optimizer='bobyqa', \n",
    "                            optCtrl=list(maxfun=2e5)))\n",
    "  HC_qt2_rnd_N_model <- glmer(as.formula(paste(HC_q_types[qt1], ' ~ ', \n",
    "                            paste(HC_q_types[setdiff(seq(1,length(q_types)), qt1)], collapse=' + '),\n",
    "                            ' + group + (1|subj) + (1|target_q)')), data=curr_qt2_rnd_N, \n",
    "                            family=binomial, control=glmerControl(optimizer='bobyqa',\n",
    "                            optCtrl=list(maxfun=2e5)))\n",
    "\n",
    "  # Saves coefficients\n",
    "  coef_S <- as.numeric(fixef(qt2_rnd_S_model)[grep(q_types[qt2], names(fixef(qt2_rnd_S_model)))])\n",
    "  coef_N <- as.numeric(fixef(qt2_rnd_N_model)[grep(q_types[qt2], names(fixef(qt2_rnd_N_model)))])\n",
    "  coef_HC_S <- as.numeric(fixef(HC_qt2_rnd_S_model)[grep(HC_q_types[qt2], names(fixef(HC_qt2_rnd_S_model)))])\n",
    "  coef_HC_N <- as.numeric(fixef(HC_qt2_rnd_N_model)[grep(HC_q_types[qt2], names(fixef(HC_qt2_rnd_N_model)))])\n",
    "\n",
    "  c(i=i, qt_dep=qt1, qt_rnd=qt2, coef_S=coef_S, coef_N=coef_N, coef_HC_S=coef_HC_S, coef_HC_N=coef_HC_N)\n",
    "}\n",
    "\n",
    "sjob <- slurm_apply(calc_glmer, data.frame(i=iter_idx, qt1=qt_pairs[1,], qt2=qt_pairs[2,], seed=rnd_seeds),\n",
    "                    jobname = 'calc_glmer',\n",
    "                    add_objects = c('q_types', 'HC_q_types', 'n_q_types', 'dep_df_S', 'dep_df_N'),\n",
    "                    nodes = n_nodes, cpus_per_node = cpus_per_node, submit = TRUE)\n",
    "all_coefs <- get_slurm_out(sjob, outtype = 'table')\n",
    "\n",
    "# Converts to matrices to make it simpler to convert to python\n",
    "all_coef_S <- array(NaN, dim=c(n_iter,n_q_types,n_q_types))\n",
    "all_coef_N <- array(NaN, dim=c(n_iter,n_q_types,n_q_types))\n",
    "all_coef_HC_S <- array(NaN, dim=c(n_iter,n_q_types,n_q_types))\n",
    "all_coef_HC_N <- array(NaN, dim=c(n_iter,n_q_types,n_q_types))\n",
    "for (j in 1:n_iter) {\n",
    "  for (qt1 in 1:(n_q_types-1)) {\n",
    "    for (qt2 in (qt1+1):n_q_types) {\n",
    "      all_coef_S[j,qt1,qt2] = subset(all_coefs, qt_dep==qt1 & qt_rnd==qt2 & i==j)[,'coef_S']\n",
    "      all_coef_N[j,qt1,qt2] = subset(all_coefs, qt_dep==qt1 & qt_rnd==qt2 & i==j)[,'coef_N']\n",
    "      all_coef_HC_S[j,qt1,qt2] = subset(all_coefs, qt_dep==qt1 & qt_rnd==qt2 & i==j)[,'coef_HC_S']\n",
    "      all_coef_HC_N[j,qt1,qt2] = subset(all_coefs, qt_dep==qt1 & qt_rnd==qt2 & i==j)[,'coef_HC_N']\n",
    "      all_coef_S[j,qt2,qt1] = subset(all_coefs, qt_dep==qt2 & qt_rnd==qt1 & i==j)[,'coef_S']\n",
    "      all_coef_N[j,qt2,qt1] = subset(all_coefs, qt_dep==qt2 & qt_rnd==qt1 & i==j)[,'coef_N']\n",
    "      all_coef_HC_S[j,qt2,qt1] = subset(all_coefs, qt_dep==qt2 & qt_rnd==qt1 & i==j)[,'coef_HC_S']\n",
    "      all_coef_HC_N[j,qt2,qt1] = subset(all_coefs, qt_dep==qt2 & qt_rnd==qt1 & i==j)[,'coef_HC_N']\n",
    "    }\n",
    "  }\n",
    "}      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls the relevant variables from the R code and converts to python native data types\n",
    "\n",
    "%Rpull all_coef_S all_coef_N all_coef_HC_S all_coef_HC_N q_types HC_q_types intact_coef_S intact_coef_N \\\n",
    "intact_coef_HC_S intact_coef_HC_N\n",
    "\n",
    "# Saves directional dependency as arrays\n",
    "intact_coef_S = np.asarray(intact_coef_S) \n",
    "intact_coef_N = np.asarray(intact_coef_N)\n",
    "intact_coef_HC_S = np.asarray(intact_coef_HC_S)\n",
    "intact_coef_HC_N = np.asarray(intact_coef_HC_N) \n",
    "\n",
    "all_coef_S = np.asarray(all_coef_S)\n",
    "all_coef_N = np.asarray(all_coef_N) \n",
    "all_coef_HC_S = np.asarray(all_coef_HC_S) \n",
    "all_coef_HC_N = np.asarray(all_coef_HC_N) \n",
    "\n",
    "# Converts to bidirectional dependency - for each pair of question types, averages the dependency\n",
    "# in the two directions\n",
    "intact_coef_S_bi = (intact_coef_S + np.transpose(intact_coef_S))/2\n",
    "intact_coef_N_bi = (intact_coef_N + np.transpose(intact_coef_N))/2\n",
    "intact_coef_HC_S_bi = (intact_coef_HC_S + np.transpose(intact_coef_HC_S))/2\n",
    "intact_coef_HC_N_bi = (intact_coef_HC_N + np.transpose(intact_coef_HC_N))/2\n",
    "\n",
    "all_coef_S_bi = (all_coef_S + np.transpose(all_coef_S, (0,2,1)))/2\n",
    "all_coef_N_bi = (all_coef_N + np.transpose(all_coef_N, (0,2,1)))/2\n",
    "all_coef_HC_S_bi = (all_coef_HC_S + np.transpose(all_coef_HC_S, (0,2,1)))/2\n",
    "all_coef_HC_N_bi = (all_coef_HC_N + np.transpose(all_coef_HC_N, (0,2,1)))/2\n",
    "\n",
    "# For each q-type pair - calculates the difference between S and N in the intact and each iteration\n",
    "intact_coef_diff = intact_coef_N - intact_coef_S\n",
    "intact_coef_HC_diff = intact_coef_HC_N - intact_coef_HC_S\n",
    "all_coef_diff = all_coef_N - all_coef_S\n",
    "all_coef_HC_diff = all_coef_HC_N - all_coef_HC_S\n",
    "\n",
    "intact_coef_diff_bi = intact_coef_N_bi - intact_coef_S_bi\n",
    "intact_coef_HC_diff_bi = intact_coef_HC_N_bi - intact_coef_HC_S_bi\n",
    "all_coef_diff_bi = all_coef_N_bi - all_coef_S_bi\n",
    "all_coef_HC_diff_bi = all_coef_HC_N_bi - all_coef_HC_S_bi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency analysis - results\n",
    "Plots the results (Figure 3 of the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines colors for plots\n",
    "\n",
    "plot_color = (0.00392157, 0.31372549, 0.40392157)\n",
    "line_plot_color =  (0.00392157, 0.31372549, 0.40392157)\n",
    "\n",
    "plot_color_N = (0.12156862745098039, 0.4666666666666667, 0.7058823529411765)\n",
    "plot_color_S = (1.0, 0.4980392156862745, 0.054901960784313725)\n",
    "\n",
    "\n",
    "# panel A - preT-postT depenedency, separately for neutral and surprising events\n",
    "fig, ax = plt.subplots(figsize=(fig_w, fig_h), dpi=plot_dpi)\n",
    "sns.distplot(all_coef_N_bi[:,1,2], ax=ax, kde=False, color=plot_color_N)\n",
    "plt.axvline(intact_coef_N_bi[1,2], color=plot_color_N, linewidth=10)\n",
    "sns.distplot(all_coef_S_bi[:,1,2], ax=ax, kde=False, color=plot_color_S)\n",
    "plt.axvline(intact_coef_S_bi[1,2], color=plot_color_S, linewidth=10)\n",
    "plt.xlabel('Dependency', fontsize=label_font)\n",
    "plt.ylabel('Num iterations', fontsize=label_font)\n",
    "plt.yticks(fontsize=tick_font)\n",
    "plt.xticks(fontsize=tick_font)\n",
    "if (save_figs):\n",
    "    fig.savefig(f'{fig_dir}dep_N_S_preT_postT.pdf')\n",
    "\n",
    "# panel B - plot of preT-postT dependency difference (N-S)\n",
    "fig, ax = plt.subplots(figsize=(fig_w, fig_h), dpi=plot_dpi)\n",
    "sns.distplot(all_coef_diff_bi[:,1,2], ax=ax, kde=False, color=plot_color)\n",
    "plt.axvline(intact_coef_diff_bi[1,2], color=line_plot_color, linewidth=10)\n",
    "plt.xlabel('Neutral/surprise dependency difference', fontsize=label_font)\n",
    "plt.ylabel('Num iterations', fontsize=label_font)\n",
    "plt.yticks(fontsize=tick_font)\n",
    "plt.xticks(fontsize=tick_font)\n",
    "if (save_figs):\n",
    "    fig.savefig(f'{fig_dir}dep_diff_preT_postT.pdf')\n",
    "\n",
    "# panel C - plot of preS-preT dependency difference (N-S)\n",
    "fig, ax = plt.subplots(figsize=(fig_w, fig_h), dpi=plot_dpi)\n",
    "sns.distplot(all_coef_diff_bi[:,0,1], ax=ax, kde=False, color=plot_color)\n",
    "plt.axvline(intact_coef_diff_bi[0,1], color=line_plot_color, linewidth=10)\n",
    "plt.xlabel('Neutral/surprise dependency difference', fontsize=label_font)\n",
    "plt.ylabel('Num iterations', fontsize=label_font)\n",
    "plt.yticks(fontsize=tick_font)\n",
    "plt.xticks(fontsize=tick_font)\n",
    "if (save_figs):\n",
    "    fig.savefig(f'{fig_dir}dep_diff_preS_preT.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculates the significance of the dependency for each of the relevant comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the p-value for the neutral/surprise dependency for each action type pair\n",
    "preS_preT_N_pval = np.mean(np.greater(abs(all_coef_N_bi[:,0,1]), abs(intact_coef_N_bi[0,1])),0)\n",
    "preT_postT_N_pval = np.mean(np.greater(abs(all_coef_N_bi[:,1,2]), abs(intact_coef_N_bi[1,2])),0)\n",
    "preS_preT_S_pval = np.mean(np.greater(abs(all_coef_S_bi[:,0,1]), abs(intact_coef_S_bi[0,1])),0)\n",
    "preT_postT_S_pval = np.mean(np.greater(abs(all_coef_S_bi[:,1,2]), abs(intact_coef_S_bi[1,2])),0)\n",
    "\n",
    "# Calculates the p-values for the neutral-surprise dependency difference\n",
    "preS_preT_diff_pval = np.mean(np.greater(abs(all_coef_diff_bi[:,0,1]), abs(intact_coef_diff_bi[0,1])),0)\n",
    "preT_postT_diff_pval = np.mean(np.greater(abs(all_coef_diff_bi[:,1,2]), abs(intact_coef_diff_bi[1,2])),0)\n",
    "\n",
    "# Compares the preT-postT dependency difference to preS-preT \n",
    "preT_postT_v_preS_preT_pval = np.mean(np.greater(abs(all_coef_diff_bi[:,1,2]-all_coef_diff_bi[:,0,1]), abs(intact_coef_diff_bi[1,2]-intact_coef_diff_bi[0,1])),0)\n",
    "\n",
    "# Prints results to screen\n",
    "print ('Results of dependency analysis:')\n",
    "print('   Neutral-version preT/postT dependency: %.3g'% preT_postT_N_pval)\n",
    "print('   Surprise-version preT/postT dependency: %.3g'% preT_postT_S_pval)\n",
    "print('   Surprise effect (neutral/surprise diff) on preT/postT dependency: %.3g'% preT_postT_diff_pval)\n",
    "print('\\n   Neutral-version preT/preS dependency: %.3g'% preS_preT_N_pval)\n",
    "print('   Surprise-version preT/preS dependency: %.3g'% preS_preT_S_pval)\n",
    "print('   Surprise effect (neutral/surprise diff) on preT/preS dependency: %.3g'% preS_preT_diff_pval)\n",
    "print('\\n   Difference in surprise effects: %.3g'% preT_postT_v_preS_preT_pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis - subjective segmentation\n",
    "\n",
    "Runs an exploratory analysis (not included in Stage 1 of the registered report) testing whether surprise constitutes a subjective event boundary. Plots the boundaries identified by participants along with the objective scene changes and the timings of the surprising targets. Supplementary Figure 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_S_boundaries = seg_S_res['subj_boundaries'] \n",
    "for m in np.arange(exp_params['n_mov']):\n",
    "    fig = plt.figure(figsize=(fig_w, fig_h), dpi=plot_dpi)\n",
    "    ax = plt.gca()\n",
    "    boundary_subset = subj_S_boundaries[subj_S_boundaries['mov_num']==m+1]\n",
    "    sns.swarmplot(x=boundary_subset['time']/1000, \n",
    "                  hue=boundary_subset['sub'], data=boundary_subset, ax=ax)\n",
    "    for t in np.array(seg_params['target_actions'][m])/1000:        \n",
    "        plt.plot([t, t], ax.get_ylim(), color=(0.7,0,0), linewidth=2.0, zorder=0)\n",
    "    for b in np.array(seg_params['mov_scene_change'][m])/1000:        \n",
    "        plt.plot([b, b], ax.get_ylim(), color='k', linewidth=2.0, zorder=0) \n",
    "    plt.xticks(fontweight='bold', fontsize=10)\n",
    "    ax.set_xlabel('Time (sec)', {'fontweight': 'bold', 'fontsize' : 14})\n",
    "    ax.set_title('Film #{}'.format(m+1), {'fontweight': 'bold', \n",
    "                 'fontsize' : 14})\n",
    "    plt.show()\n",
    "    if (save_figs):\n",
    "        fig.savefig(f'{fig_dir}boundary_target_match_S_mov{{}}.eps'.format(m+1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Analyses\n",
    "Code for creating Supp. Figures 1-2 - plotting boundaries identified by participants watching the neutral version of the film relative to objective scene changes. Data for Supplementary tables calculated as part of the main analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a plot with the scene changes and all boundaries indicated by\n",
    "# by all subjects (here without the target actions as they are all neutral)\n",
    "subj_boundaries = seg_res['subj_boundaries'] \n",
    "for m in np.arange(exp_params['n_mov']):\n",
    "    fig = plt.figure(figsize=(fig_w, fig_h), dpi=plot_dpi)\n",
    "    ax = plt.gca()\n",
    "    boundary_subset = subj_boundaries[subj_boundaries['mov_num']==m+1]\n",
    "    sns.swarmplot(x=boundary_subset['time']/1000, \n",
    "                  hue=boundary_subset['sub'], data=boundary_subset, ax=ax)\n",
    "    for b in np.array(seg_params['mov_scene_change'][m])/1000:        \n",
    "        plt.plot([b, b], ax.get_ylim(), color='k', linewidth=2.0, zorder=0) \n",
    "    plt.xticks(fontweight='bold', fontsize=tick_font)\n",
    "    ax.set_xlabel('Time (sec)', {'fontweight': 'bold', 'fontsize' : label_font})\n",
    "    ax.set_title('Film #{}'.format(m+1), {'fontweight': 'bold', \n",
    "                 'fontsize' : label_font})\n",
    "    plt.show()\n",
    "    if (save_figs):\n",
    "        fig.savefig(f'{fig_dir}boundary_match_mov{{}}.eps'.format(m+1))\n",
    "    \n",
    "\n",
    "# Plots all peak heights, to verify the distribution is divided into two clear \n",
    "# clusters, and the identification of boundaries isn't dependent on the \n",
    "# precise threshold chosen (of the minimal number of participants that ) \n",
    "all_peak_heights = seg_res['all_peak_heights']\n",
    "fig = plt.figure(figsize=(fig_w, fig_h), dpi=plot_dpi)\n",
    "ax = plt.gca()\n",
    "sns.swarmplot(x=np.ones(len(all_peak_heights)), y=all_peak_heights)\n",
    "plt.xticks([])\n",
    "plt.yticks([5,10,15], fontweight='bold', fontsize=tick_font)\n",
    "ax.set_ylabel('#participants', {'fontweight': 'bold', 'fontsize' : label_font})\n",
    "if (save_figs):\n",
    "        fig.savefig(f'{fig_dir}boundary_peaks.eps')\n",
    "\n",
    "# Prints the average distance between the marked boundaries and the scene\n",
    "# changes, the maximal distance and the number of scene changes that\n",
    "# weren't identified by subjects.\n",
    "subj_mean_boundaries = seg_res['subj_mean_boundaries']\n",
    "n_no_match = sum(subj_mean_boundaries['time'].isna())\n",
    "mean_delay = np.mean(subj_mean_boundaries['scene_dist'])\n",
    "max_dist = np.max(abs(subj_mean_boundaries['scene_dist']))\n",
    "print('Number of unidentified scenes: %.3g'% n_no_match)\n",
    "print('Mean delay between scene change and boundary: %.3g'% mean_delay)\n",
    "print('Max (abs) distance between scene change and boundary: %.3g'% max_dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
